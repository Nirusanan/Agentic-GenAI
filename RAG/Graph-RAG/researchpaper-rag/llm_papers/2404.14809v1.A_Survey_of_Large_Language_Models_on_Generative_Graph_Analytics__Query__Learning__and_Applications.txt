A Survey of Large Language Models on Generative
Graph Analytics: Query, Learning, and Applications
Wenbo Shang
Department of Computer Science
Hong Kong Baptist University
Hong Kong, China
cswbshang@comp.hkbu.edu.hkXin Huang
Department of Computer Science
Hong Kong Baptist University
Hong Kong, China
xinhuang@comp.hkbu.edu.hk
Abstract —A graph is a fundamental data model to represent
various entities and their complex relationships in society and
nature, such as social networks, transportation networks, finan-
cial networks, and biomedical systems. Recently, large language
models (LLMs) have showcased a strong generalization ability
to handle various NLP and multi-mode tasks to answer users’
arbitrary questions and specific-domain content generation.
Compared with graph learning models, LLMs enjoy superior
advantages in addressing the challenges of generalizing graph
tasks by eliminating the need for training graph learning models
and reducing the cost of manual annotation. In this survey, we
conduct a comprehensive investigation of existing LLM studies
on graph data, which summarizes the relevant graph analytics
tasks solved by advanced LLM models and points out the
existing remaining challenges and future directions. Specifically,
we study the key problems of LLM-based generative graph
analytics (LLM-GGA) with three categories: LLM-based graph
query processing (LLM-GQP), LLM-based graph inference and
learning (LLM-GIL), and graph-LLM-based applications. LLM-
GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge
graph (KG) based augmented retrieval , while LLM-GIL focuses
on learning and reasoning over graphs, including graph learning ,
graph-formed reasoning and graph representation . We summarize
the useful prompts incorporated into LLM to handle different
graph downstream tasks. Moreover, we give a summary of LLM
model evaluation, benchmark datasets/tasks, and a deep pro and
cons analysis of LLM models. We also explore open problems
and future directions in this exciting interdisciplinary research
area of LLMs and graph analytics.
Index Terms —Graph, LLMs, GNNs, Prompt, Survey
I. I NTRODUCTION
Large language models (LLMs) possess billions of parame-
ters and have been trained on extensive corpora using training
strategies like instruction tuning [1] [2] and Direct Preference
Optimization(DPO) [3], enabling them to exhibit powerful
reasoning and semantic representation capabilities, thereby
advancing AI intelligence closer to human levels. Undoubt-
edly, LLMs currently serve as the foundation model for NLP
tasks [4] [5] [6], showcasing strong generalization abilities to
handle various NLP tasks such as question answering [7] [8],
machine translation [9], code generation [10] [11], etc. LLMs
have demonstrated extensive common knowledge and robust
semantic comprehension abilities, fundamentally transforming
existing text-processing workflows. While initially designed
for text data, LLMs are increasingly being utilized for tasks
LLM-GGALLM-GQPLLM-GILGraph-LLM-based applications
GraphsLLMs+
Graph Queries
LLMsAnswersLLMs
Graphs
GraphsGraph representationGraph learning tasksGraph reasoning
KGsFig. 1: Illustration of the LLM-GGA domain. LLM-GGA do-
main includes three principal components: LLM-based graph
query processing (LLM-GQP), which necessitates the melding
of graph analytics techniques and LLM prompts for query pro-
cessing; LLM-based graph inference and learning (LLM-GIL),
focusing on learning and reasoning over graphs; Graph-LLM-
based applications that employ the graph-LLM framework to
address non-graph tasks, such as recommendation systems.
beyond language processing, aiming to leverage the robust ca-
pabilities of LLMs across different tasks, showcasing superior
performance.
Graphs, as structured data, play a crucial role in various real-
world application scenarios, including the citation networks
[12], social networks [13], molecular graphs [14], web links
[15], and to name a few. Various graph analytics tasks have
been studied to show their usefulness, e.g., node classification,
link prediction, subgraph mining, influence maximization, and
so on. Their versatility and ability to capture complex rela-
tionships have made graphs indispensable tools in academic
research and industry platforms. Recently, one kind of graph-
based learning model, graph neural network (GNN) [16] [17],
has been widely studied and applied to solve challenging graph
tasks. The GNN models utilize recursive message passing
[18] and aggregation mechanisms [19] among nodes to derive
representations of nodes, edges, or entire graphs, which have
been used for various downstream tasks. This is thanks to
the strong ability of GNN models to capture both graph
structure and node features. However, GNNs exhibit weak
generalization capabilities [20] [21] [22], requiring retraining
for different graph tasks and showing limited transfer ability.arXiv:2404.14809v1  [cs.CL]  23 Apr 2024In other words, no universal graph foundation model could be
easily generalized to handle various types of graph tasks.
Therefore, whether LLMs’ powerful reasoning, semantic
representation, and generalization capabilities can be applied
to address graph tasks, leading to the inspiration of a graph
foundation model, is the core of current efforts in leveraging
existing large language models for graph-related tasks. In one
word, can LLMs solve graph data tasks? More specifically,
we study three detailed questions: (a) what specific graph
tasks can LLMs answer? (b) How do LLMs tackle these
tasks? (c) What is the effectiveness of LLM-based methods
in solving these tasks compared with the existing graph-based
approaches?
To address the above question, this survey conducts a
comprehensive study of existing relevant work on graph an-
alytics and LLMs, focusing on exploring the key issue of
the LLM-based generative graph analytics (LLM-GGA) field.
Drawing from a thorough investigation of the LLM-GGA
domain, we offer a structured and methodical analysis that
delineates the field into three principal components: LLM-
based graph query processing (LLM-GQP), which necessitates
the melding of graph analytics techniques and LLM prompts
for query processing; LLM-based graph inference and learning
(LLM-GIL), focusing on learning and reasoning over graphs;
and lastly, graph-LLM-based applications that employ the
graph-LLM framework to address non-graph tasks, such as
recommendation systems. The framework is shown in Figure
1.
We categorize these three main components into a total
of six directions to provide a guideline for researchers to
conduct more in-depth studies. LLM-GQP includes graph
understanding and KG-based augmented retrieval directions.
LLM-GIL covers graph learning, graph-formed reasoning, and
graph representation directions. The sixth direction is graph-
LLM-based applications. The following section details these
six directions:
•Graph understanding tasks. This research direction is
studying whether LLMs can solve graph algorithm prob-
lems, exploring whether LLMs can comprehend graph
structures to conduct graph mining and graph search. Cur-
rent methods have primarily explored LLMs’ understand-
ing of graph structures, such as shortest path, clustering
coefficient computation [23] [24], and more complex
problems like maximum flow and Hamilton path [25] [26]
[27]. Two main methods are introduced: prompting and
supervised fine-tuning (SFT). The prompting methods
explore the LLM’s current structural understanding abil-
ity through query processing. Meanwhile, SFT methods
enhance LLMs’ structure understanding capability by
tuning it on specific graph datasets. However, many more
tasks are yet to be explored, such as the community
search, keyword search, subgraph pattern mining, and
other NP-hard complex graph problems [28] [29].
•Graph learning tasks. This direction explores whether
LLMs can combine graph structure and attributes for
learning, extracting features of nodes, edges, and graphs,and understanding the semantic information of graphs,
for example, tasks like node classification, graph classi-
fication, and GQL generation [30] [31] [32] [33]. There
are two main pipelines: LLM-GNN pipelines and LLM
pipelines. LLMs can leverage their powerful reasoning
ability and vast knowledge repository to enhance GNNs
and also can predict results directly.
•Graph-formed reasoning. This direction explores how
LLMs use graph structures to simulate human thinking
during reasoning [34] [35] [36], enabling them to solve
more complex reasoning problems such as algorithmic,
logical, and mathematical tasks. Graph-formed reasoning
involves two types of reasoning: think on the graph and
verify on the graph. Think on the graph refers to LLMs
deriving the final conclusion through the graph structure.
Verify on the graph refers to verifying the correctness of
the LLMs’ intermediate or final outputs through the graph
structure.
•Graph representation. This direction explores enhanc-
ing graph representation with LLMs, particularly for Text
Attribute Graphs (TAGs). LLMs’ strong text representa-
tion capabilities allow text embeddings to capture deeper
semantic nuances. However, the key challenge in this
area remains how to capture and integrate graph structure
into graph representation effectively [37] [38] [39]. There
are three forms of graph representation: graph embed-
ding, graph-enhanced text embedding, and graph-encoded
prompts. Graph embedding methods transform a graph
into a sequential format for LLM processing. Graph-
enhanced text embedding methods integrate structure into
text embedding, where the integration method can be
concatenation. Graph-encoded prompts focus on the way
a graph is described within prompts.
•Knowledge Graph (KG) based augmented retrieval.
This direction investigates the relationship between LLMs
and Knowledge Graphs (KGs). With the emergence of
LLMs, discussions have arisen regarding the potential
replacement of KGs [40] [41] [42] [43]. Consequently,
this paper discusses the limitations of LLMs in processing
factual knowledge, evaluates strategies for improving
LLM efficacy via KG-based augmented retrieval, and
investigates potential avenues for future advancements in
this field.
•Graph-LLM-based applications. This part explores the
tasks where graph-LLM-based methods can be applied
for useful downstream application [44] [45] [46], such as
recommendation systems, conversational understanding,
and so on.
We comprehensively analyze these six research directions
of LLM-GGA to provide valuable definitions and highlighted
methodologies. We also highlight the pros and cons of these
methods and showcase future directions. To further explore the
capabilities of LLMs reliably, this paper uses the prompting
method to test the effectiveness of LLMs in tasks such as
graph structure understanding, graph learning, and graph-formed reasoning. Details of the prompts and results obtained
during testing are also provided. Additionally, we refine and
compile commonly used and effective prompts for graph-
related tasks, assisting researchers in conducting experiments.
Furthermore, this paper also organizes and introduces the
code for existing popular methods, benchmarks for LLM-GGA
tasks, and evaluations measuring LLM performance in graph
tasks to facilitate future research.
Our contributions and the identified challenges for future
research. In this paper, we provide a comprehensive survey of
the state-of-the-art work on LLMs applied to graph data. We
begin by delineating six critical directions in the field of LLM-
GGA: graph structure understanding, graph learning, graph-
formed reasoning, graph representation, KG-based augmented
retrieval, and graph-LLM-based applications. This categoriza-
tion clarifies the current work and offers a guideline for future
research endeavors. In each direction, we propose a structured
introduction and summarization using vivid examples and
offer suitable specific pipelines. We analyze the advantages
and limitations of current methodologies and suggest avenues
for future research. Furthermore, we organize resources related
to benchmarks, evaluations, and code links within the LLM-
GGA domain to facilitate further investigation by researchers.
Lastly, we identify the fundamental challenges in the LLM-
GGA field, which are the primary obstacles to advancing LLM
in solving graph tasks, including the fundamental issue of how
sequential LLM handles structural graph data, the efficiency
issue of large-scale graph data, and the NP-hard problems of
complex graph analytics. This clarification guides the research
direction for future work on LLM-GGA.
Roadmaps . The organization of this paper is as follows. We
first present the fundamental preliminaries and summarize
the graph description language, which converts graphs into
sequences before inputting them into LLMs in Section II.
Then, we introduce six tasks of LLM-based graph analytics
one by one. We present the graph structure understanding
direction in Section III, graph learning direction in Section IV,
graph-formed reasoning in Section V, graph representation in
Section VI, KG-based augmented retrieval in Section VII and
graph-LLM-based applications in Section VIII. In the above
six directions, we clarify the tasks that LLMs can perform,
discuss the methodologies, conduct a comparative analysis,
and propose guidelines and principles in this direction. Fol-
lowing this, Section IX introduces the popular datasets and
new datasets for solving the above tasks and also provides
metrics for evaluating LLMs or tasks in different directions. In
Section X, we identify and discuss the current and upcoming
challenges that LLM-GGA faces and future directions. Finally,
our conclusions are presented in Section XI.
II. P RELIMINARY
In the subsequent section, we will initially introduce graph
data, proceed to discuss GNNs as a paradigm of graph-
based learning models, then introduce LLMs and distinguish
LLMs and PLMs, and ultimately introduce graph descriptionlanguage, which can transform the graph into sequential data
as the input of LLMs.
A. Graph
Graph data represents complex relationships through nodes
and edges, where nodes represent entities and edges represent
their interconnections. This structure excels at modeling intri-
cate networks such as social, biological, and transportation
systems. It enables analyses like community detection and
shortest path calculations, offering critical insights into the
dynamics of various systems. Formally, a general graph can
be represented as G= (V,E), where VandEdenote the set
of nodes and edges. V={v1, v2, ..., v n}where the number
of nodes is |V|and|V|=n.E={eij}where the number of
edges is |E|andeijis an edge from vitovj.
B. Graph Neural Network
Graph Neural Networks (GNNs) [16] [17] are a type of deep
learning model that can handle graph-structured data. The goal
of these GNNs is to learn representations for each node, which
are computed based on the node’s own features, the features of
the edges connected to it, the representations of its neighbors,
and the features of its neighboring nodes,
hl
v=AGGR (hl−1
v,{hl
u−1 :u∈Nv};θl) (1)
where hl
vrepresents the representation of node vin the l-th
layer. AGGR denotes the aggregation function that aggregates
the representations of neighboring nodes from the previous
layer. For the tasks that focus on individual nodes, e.g.,
node classification, the learned representations can be used
directly to accomplish specific objectives. However, for the
tasks that consider the entire graph, e.g., graph classification,
a global representation can be obtained by pooling or applying
other methods to the representations of all nodes. This global
representation can then be used to perform the corresponding
tasks.
C. Large Language Models
Currently, there is no precise definition for Large Language
Models (LLMs). However, according to the pioneering surveys
[47] [48] on LLMs, a distinction can be made between LLMs
and Pre-trained Language Models (PLMs). LLMs are large
language models with billion-level parameters that are pre-
trained on massive amounts of data, such as Llama [5] and
ChatGPT. Conversely, PLMs are pre-trained language models
with million-level parameters that can be more easily fine-
tuned on task-specific data. While LLMs and PLMs share
similarities in their pre-training process, the former is char-
acterized by its larger size and ability to generate human-like
text. Thus, it is essential to consider the potential implications
of using LLMs in various applications.
D. Graph Description Language
Graphs are represented in the structured data in arbitrary
shapes, while LLMs typically process sequential data, such
as the text as a sequence of words. To bridge this gap,Graph Structure Understanding Tasks
14320Graph  Size CalculationGiven <graph>, what is the number of nodes and edges in this graph? Please answer with the number of nodes: X, number of edges: X. 14320Degree CalculationGiven <graph>, what is the degree of node 4?  Or, like, find the node degree of node [given node] in the given graph.14320Connected Nodes SearchGiven <graph>. Is node 3 the 1-hop neighbor of node 4? List the answers after “Ans:” in the format of [Yes, No,]. 14320Edge ValidationGiven <graph>. Is there an edge between node 1 and node 2?14320Path SearchGiven <graph>. Simple path: Find a single path from node 0 to node 4 connected by edges in the given graph. Shortest path: Give the shortest path from node 0 to node 4.14320Attribute RetrievalGiven <graph>, what is the title of node 0?Abstract: Text in curve orientation, despite being one of the common…Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.14320Graph DensityGiven <graph>, what is the density of the given graph?14320EccentricityGiven <graph>, what is the eccentricity of the node 0?14320Pattern matchingGiven <graph>, in the given graph, the triangle must be connected by three edges, list the triangle after ”Ans:” in the format of [0-1-2]14320Topological SortingIn a directed graph with 5 nodes numbered from 0 to 4: node 0 should be visited before node 1, ... Q: Can all the nodes be visited? Give the solution.12100Bipartite Graph MatchingThere are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 0 to 2. Each applicant is interested in some of the jobs. Each job can only accept one applicant and a job applicant can be appointed for only one job. Applicant 0 is interested in job 1, ... Q: Find an assignment of jobs to applicants in such that the maximum number of applicants find the job they are interested in.14320Hamilton PathGiven <graph>, is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path, adjacent nodes must be connected with edges. 14320Maximum FlowIn a directed graph with 5 nodes numbered from 0 to 4, and the edges are: an edge from node 0 to node 1 with capacity 10... Q: What is the maximum flow from node 0 to node 3?101559(a)(b)(c) (d) (e)
(f)(g)(h)(j)
(k)(l) (m)(n) 14320Graph DiameterGiven <graph>, what is the diameter of the given graph?(i)Fig. 2: Graph Structure Understanding tasks.
the graph description language (GDL) transforms the graph
into sequential data, which can be inputted into an LLM.
Specifically, GDL aims to convert graphs into sequential data
while retaining the structure and unique attributes of the graph.
This conversion allows the graph’s information to be fed into
an LLM for processing. There are several graph description
languages:
•Text description. Graph structure can be described using
words such as ‘Node 1 is connected to Node 2’ and
‘There are three nodes connected to Node 1’.
•Adjacency list. An adjacency list represents each vertex
in the graph with the collection of its neighbouring
vertices or edges. Node A is connected with node B and
node C can be denoted as N(v) ={B, C}.
•Edge list. An edge list represents the edge connections
between two nodes in the graph. (A, B) indicates a
connection between nodes A and B.
•GML. Graph Modelling Language [49] consists of an
unordered sequence of node and edge elements enclosed
within ‘[·]’.
•GraphML. Graph Markup Language [50] consists of
XML containing a graph element and an unordered
sequence of node and edge elements.
•SQL. Several specialized SQL languages are designed
specifically for working with graph data. These languages
are also capable of serving as graph description lan-
guages. Some notable examples include Cypher [51], a
query language developed by Neo4j, and Gremlin [52],
SPARQL [53], and GSQL [54]. They combine SQL-
like syntax with graph-specific constructs and algorithms,
making them suitable for complex graph analytics tasks.
•Multi-modality encoding. Except for text description,graph structure can also be represented using image
description and motif description. The graph can be visu-
alized as an image and inputted into an LLM to process
images. Alternatively, motifs such as stars, triangles, or
clique patterns can represent the graph structure as input
into an LLM.
•Encode as a story. The graph can be encoded within
a specific context, such as a friendship, co-authorship,
social network, politician, or expert. For example, the
connections between nodes can represent friendship re-
lationships. We can assign names to the nodes, such as
‘David’ and ‘Alice’.
Notably, (1) different graph description languages can yield
different results of LLMs. Therefore, it is suggested to test
with multiple GDLs and select the one with the best experi-
mental results. (2) If needed, the LLM’s output form can be
specified along with GDLs in the prompt. LLMs often generate
excessive reasoning processes that may be unnecessary, so
standardizing the LLM’s output can be beneficial.
III. G RAPH STRUCTURE UNDERSTANDING TASKS
Graph structure understanding tasks evaluate whether LLMs
can comprehend graph structures. Simple tasks include the
queries of neighbors, shortest paths, connectivity, the calcu-
lation of graph radius, and the clustering coefficient. More
complex tasks include solving maximum flow problems and
performing topological sorting. These tasks need LLMs to
comprehend graph structures locally and globally, as shown in
Figure 2. In this section, we present 21 graph understanding
tasks along with their definitions. Subsequently, we elaborate
on the two main methods currently used to address graph
structure understanding tasks: prompting and supervised fine-
tuning LLMs.Task Prompts
Graph Data Loading The structure of the [file path] molecular graph of the benzene ring contains a hexagon.
Graph Size Detection Given [graph], what is the number of nodes and edges in this graph? Please answer with the number of nodes:
X, number of edges: X.
Degree Detection Given [graph], what is the degree of node 4? Or, find the node degree of node [given node] in the given graph.
Connected Nodes Given [graph]. Is node 5 the 1-hop neighbor of node 4? List the answers after “Ans:” in the format of [Yes, No,].
Edge Detection Given [graph]. Is there an edge between node 1 and node 2?
Path Simple path: Given the undirected graph with the specified nodes and edges, nodes: [0, 1, 2, 3, 4], edges: [(0,
1), (1, 4), (1, 3), (4, 3), (3, 2)], find a single path from node 1 to node 2 connected by edges in the given graph.
Shortest path: Given the directed graph with the specified nodes and edges, nodes: [0, 1, 2, 3, 4], edges: [(0, 1),
(1, 4), (1, 3), (4, 3), (3, 2)], give the shortest path from node 0 to node 4.
Attribute Retrieval Given [graph]. What is the title of node 0?
Graph Density Given [graph]. What is the density of the given graph?
Eccentricity Given [graph]. What is the eccentricity of the given graph?
Graph Radius Given [graph]. What is the radius of the given graph?
Graph Diameter Given [graph]. What is the diameter of this graph?
Graph Periphery Given [graph]. What is the periphery of this graph? Or What are the nodes included by the periphery of the given
graph?
Clustering Coefficient Computing Given [graph]. What is the clustering coefficient of [given node]?
TABLE I: Prompts for Graph Structure Understanding Tasks, where [graph] is the input of the data.
A. Task Introduction
1) Graph size calculation: Graph size refers to the number
of nodes and edges in a graph. Given a general graph G=
(V,E), the graph size detection task is to detect the |V|and|E|
inG. Through this task, LLMs are expected to understand the
fundamental structure of a graph accurately. Given a prompt
describing the graph and asking related queries, LLMs are
supposed to determine |V|and|E|, as shown in Figure 2 (a).
2) Degree calculation: The degree detection task involves
determining the degree of a specific node in a graph. The
neighbors of node vcan be denoted as N(v) ={u|(u, v)∈
E(v)}, where E(v)is the edge set including edges connected to
v. The degree of viis the number of its neighbors in G, which
can be denotes as degG(vi) =|N(vi)|. Through this task,
LLMs are expected to comprehend the context surrounding vi
and identify N(vi)accurately. By inputting a prompt about
viandG, LLMs are expected to calculate the degree of the
node. This task is shown in Figure 2 (b).
3) Connected nodes search: The connected nodes detection
task involves finding all the nodes in NG(vi)ofviinG. Given
the prompt about G, LLMs are expected to analyze the local
structure of the given node viand determine NG(vi), as shown
in Figure 2 (c).
4) Edge validation: The edge detection task refers to
whether there exists an edge eijoreijbetween viandvi.
Through this task, LLMs are expected to accurately identify
the connectivity between nodes and understand the localstructure of nodes. Given the prompt about the neighbors of
vito the LLMs, LLMs will likely indicate whether eijoreij
exists, as shown in Figure 2 (d).
5) Path search: We consider two types of paths, including
the simple path and the shortest path, as shown in Figure 2
(e). Given a graph G={V,E}, the simple path task involves
detecting whether there exists a path (vi, ..., v j)between a
source node viand a target node vjinG. In other words, it
is about finding a simple path (vi, ..., v j)between viandvj
without specific requirements. This task evaluates the ability of
LLMs to traverse a graph and understand its structure. Given
the prompt about Gto LLMs, the goal is to return a simple
path from vitovj.
Given a weighted directed acyclic graph G={V,E}with
each edge e∈ E has a non-negative weight w(e), the shortest
paths task involve finding a path p= (e1, e2, . . . , e n)from a
source node to a target node in Gsuch that the sum of the
weights of edges w(p) =Pn
i=1w(ei)is minimized. LLMs
can evaluate the length of the shortest path and identify the
qualified paths. This task can be further divided into three
objectives: 1. Finding the shortest path between two nodes. 2.
Finding all the shortest paths for all paired nodes. 3. Finding
the average length of all the shortest paths. This task assesses
whether the LLM can effectively determine the shortest route
between two specified nodes within the graph.
6) Attribute retrieval: The attribute retrieval task involves
retrieving detailed information related to nodes, such as theTask Prompts
Graph Partition In the academic collaboration network dblp, scholar #355233 is involved in [TBR] local community formed by his/her
collaborators.
Graph Searching According to the Freebase knowledge graph, the relation between entity /m/027rn and entity /m/06cx9 is [TBR].
Pattern matching Triangle: find a single triangle containing node X. Or in the given graph, the triangle must be connected by three edges,
list the triangle after ”Ans:” in the format of [0-1-2]. Cliques: find all the cliques with Nnodes in the given graph, list all
the cliques after ”Ans:” in the format of [0-1-2] and separate the answers by a comma. Wedge Centering find a single
wedge containing node X in the given graph, node X must be the center of this wedge, list the wedge after ”Ans:” in the
format of [0-1-2].
Cycle Check In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge. The nodes are numbered
from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0) Q: Is there a cycle in this graph?
Topological Sort In a directed graph with 5 nodes numbered from 0 to 4: node 0 should be visited before node 4, ... Q: Can all the nodes
be visited? Give the solution.
Maximum Flow In a directed graph with 5 nodes numbered from 0 to 4, and the edges are: an edge from node 0 to node 1 with capacity
10... Q: What is the maximum flow from node 0 to node 3?
Bipartite Graph Matching There are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 0 to 2. Each applicant is interested in some
of the jobs. Each job can only accept one applicant and a job applicant can be appointed for only one job. Applicant 0 is
interested in job 1, ... Q: Find an assignment of jobs to applicants in such that the maximum number of applicants find
the job they are interested in.
Hamilton Path Given [graph], is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path,
adjacent nodes must be connected with edges.
Graph Neural Networks Given [graph]. Embeddings: node 0: [1,1], ... In a simple graph convolution layer, each node’s embedding is updated by
the sum of its neighbors’ embeddings. Q: What’s the embedding of each node after one layer of simple graph convolution
layer?
Dynamic Graph In an undirected dynamic graph, (u, v, t) means that node u and node v are linked with an undirected edge at time t. Your
task is to answer when two nodes are first connected in the dynamic graph. Two nodes are connected if there exists a
path between them. Given an undirected dynamic graph with the edges [(0, 1, 0), (1, 2, 1), (0, 2, 2)]. When are node 0
and node 2 first connected?
TABLE II: Prompts for Graph Structure Understanding Tasks, where [graph] is the input of the data. [TBR] means to be
reasoned by LLMs.
attributes of a node. For example, in a citation network, LLMs
are tasked with retrieving specific attributes of a node, such
as the title, abstract, or author of a paper. Given the prompt
aboutGand detailed attribute information, LLMs are expected
to retrieve the required information, as shown in Figure 2 (f).
7) Graph density: Graph density represents the ratio be-
tween the number of edges present in a graph and the
maximum number of edges that the graph can have. For an
undirected simple graph G={V,E}, the graph density is
defined as:
D=2|E|
|V|(|V| − 1)(2)
For a directed simple graph, the graph density is defined as:
D=|E|
|V|(|V| − 1)(3)
This task requires LLM to calculate the density of a given
graph and assess its understanding of the entire graph, as
shown in Figure 2 (g).
8) Eccentricity: The eccentricity of a node in a graph is
defined as the length of the longest shortest path starting at that
node. The eccentricity of one node: this task requires LLMsto answer the eccentricity of a given node. The eccentricity of
many nodes: this task requires LLMs to answer the eccentricity
of a subset of nodes or all the nodes in the graph, as shown
in Figure 2 (h).
9) Graph radius: Based on the eccentricity of nodes, the
radius of a graph is the minimum eccentricity of any vertex in
the graph. LLMs can calculate the radius of the given graph
with the description of the graph.
10) Graph center: The center of a graph is the set of
vertices of graph eccentricity equal to the graph radius. Based
on the eccentricity task and graph radius task, LLMs should be
given the graph information and asked to calculate the graph
center.
11) Graph diameter: Based on the shortest path, the diam-
eter of a graph is the length of the shortest path between the
most distant nodes. LLMs can calculate the graph’s diameter
with the given graph information, as shown in Figure 2 (i).
12) Graph periphery: Based on the graph eccentricities and
graph diameter, the graph periphery is a set of vertices that
have graph eccentricities equal to the graph diameter. LLMs
can answer questions related to the graph periphery using the
given graph information.Fig. 3: Examples for Path Task with GPT3.5 - Graph Structure
Understanding Tasks.
Fig. 4: Examples for Maximum Flow Task with GPT3.5 -
Graph Structure Understanding Tasks.
Fig. 5: Examples for Bipartite Graph Matching Task with
GPT3.5 - Graph Structure Understanding Tasks.13) Clustering coefficient computing: The clustering coef-
ficient is a measure of how connected a vertex’s neighbors are
to one another. We define the edges among neighbors of vi
as{ejk:vj, vk∈ NG(vi), ejk∈ E} . For directed graphs, the
clustering coefficient is defined as:
Ci=|{ejk:vj, vk∈ NG(vi), ejk∈ E}|
|NG(vi)||NG(vi)−1|(4)
For undirected graphs, the clustering coefficient is defined as:
Ci=2|{ejk:vj, vk∈ NG(vi), ejk∈ E}|
|NG(vi)||NG(vi)−1|(5)
LLMs can calculate the clustering coefficient as a measure of
the degree to which nodes in a graph tend to cluster together.
14) Graph partition: This task is an online social network
reasoning task, which is to infer the community structure of
an online social network by partitioning users into different
clusters based on their interaction information. Each cluster
represents a social community formed by users who interact
with each other frequently. LLMs partition the users of the
social network based on user social interaction patterns and
generate the resulting cluster assignments.
15) Graph searching: This task is a knowledge graph
reasoning task, which involves inferring relationships between
entities based on their information or inferring connected
entities based on the information of entities and relationships.
Specifically, LLM takes entities or relationships as input
and searches for relevant entities or relationships to generate
output.
16) Pattern matching: This task is to identify star, wedge,
triangle, or clique patterns that contain a target node. The
target node can be defined as the center of the pattern.
Alternatively, the task can involve identifying whether these
patterns exist in a given graph and determining the number
of occurrences. Given a description of the LLM graph, the
goal is for LLM to identify different patterns and provide the
corresponding answers, as shown in Figure 2 (j).
17) Cycle validation: This task is to determine whether a
graph contains a cycle. Given G={V,E}, a cycle is a non-
empty trail with a vertex sequence (v1, v2, ..., v n, v1). Given
the graph information, LLM is asked to determine whether
this graph has a cycle.
18) Topological sorting: Topological sorting of a directed
graph G={V,E}refers to a linear ordering of its nodes,
where each node comes before all the nodes it points to,
for example, there exists a directed edge eijfrom vitovj,
vicomes before vjin the ordering. The resulting array of
node ordering is called topological ordering. LLM is required
to generate a valid topological sorting for the given directed
graph, and there may be multiple valid solutions, as shown in
Figure 2 (k).
19) Maximum flow: Given a capacity constraint, the max-
imum flow problem involves finding the maximum flow that
can be sent through pipes, channels, or other pathways in a
network. Define a flow as fijfrom vitovjand the capacity
on edge eijascij. Given the capability constraints, fij≤cijManual promptSelf-promptingAPI call prompts………………Frozen LLMManual prompt: Given <graph>, what is the number of nodes and edges in this graph? Please answer with the number of nodes: X, number of edges: X. ………………Frozen LLMInstructor: You are a brilliant graph master that can handle anything related to graphs like retrieval, detection and classification.Graph description language: GML, GraphML, etc. Query: What is the clustering coefficient of node X?New contexts: Text description of input graph generated by LLM itself.Final output: The clustering coefficient of node X is …………………Trainable LLMRegular prompt: What is the diameter of the binomial tree?API call prompt: The diameter of the binomial tree is Cerulean [GR(GL(“gpr”, “binomial_tree”), “toolx:diameter”) →r]Fig. 6: Promoting methods in graph structure understanding tasks. There are three categories: manual prompts, self-prompting,
and API call prompts.
for all eij. Meanwhile,P
fij>0fij=P
fji>0fjifor∀viexcept for
the source and the target {s, t}Given a network graph, LLM
generates a path that maximizes the flow from the source to
the sink, as shown in Figure 2 (l).
20) Bipartite graph matching: A bipartite graph is a type
of graph where the nodes can be divided into two disjoint sets,
UandV, such that there are no adjacent nodes within each set.
A matching in a bipartite graph is a set of edges where no two
edges share an endpoint. In a maximum matching, if any edge
is added, it is no longer a matching. For a given bipartite graph,
there can be multiple maximum matchings. LLM can generate
a solution that finds the maximum matching, as shown in
Figure 2 (m).
21) Hamilton Path: In an undirected graph, a Hamiltonian
path is a path in the graph that visits each vertex exactly once.
Given an undirected graph, the task is for LLM to find a valid
Hamiltonian path, as shown in Figure 2 (n).
B. Graph Structure Understanding Methods
The rise of LLMs has sparked researchers’ interest in
exploring their powerful text processing and generalization
capabilities for graph reasoning. Therefore, existing efforts
have introduced various benchmarks to test LLMs’ graph
reasoning potential, aiming to explore their capacity to address
graph-related problems. Prompting methods have emerged as
the primary approach to assess LLMs’ understanding of graph
structures, with some studies also focusing on fine-tuning
LLMs to enhance their graph reasoning abilities. Thus, the
following two main methods are introduced: prompting method
andfine-tuning LLMs .
1)Prompting method :The prompting method [55] can
be categorized into three main types: manual prompt, self-
prompting, and API call prompt, as shown in Figure 6.
Most studies utilize manual prompts, where carefully crafted
prompts guide LLMs to comprehend graph structures better
and understand the objectives of graph tasks, thereby leading
to improved performance on graph-related tasks.
Manual prompts. NLGraph [27] introduces a benchmark
aiming to assess the understanding capabilities of LLMs in
processing textual descriptions of graphs and translating theminto conceptual spaces. This benchmark covers various graph
reasoning tasks like connectivity, shortest path, maximum flow,
and graph neural network construction, with three difficulty
levels (easy, medium, hard) based on graph size and density.
Meanwhile, the number of nodes n=|V|and the probability
pcontrol edge generation, allowing manipulation of graph size
and density for a more reliable evaluation of LLM potential
in graph comprehension.
Next, to guide LLMs in solving these graph tasks, two
prompt methods are proposed by NLGraph [27]: build-a-graph
prompting and algorithmic prompting.
Prompt III-1: Build-a-Graph Prompting. Build-a-Graph
prompting method is to guide LLMs to conceptual grounding
by adding one sentence shown as red words below:
Prompt III-1: Build-a-Graph Prompting
Given <graph description >.Let’s construct a graph
with the nodes and edges first. Q: What is the degree
of node 4?
Prompt III-2: Algorithmic Prompting. The algorithmic
prompting method is designed to guide LLMs to engage in
algorithmic reflection and thinking by adding the details of
the algorithm shown as red words below:
Prompt III-2: Algorithmic Prompting
We can use a Depth-First Search (DFS) algorithm to
find the shortest path between two given nodes in an
undirected graph.
The basic idea is to start at one of the nodes and use
DFS to explore all of its adjacent nodes. At each
node, you can keep track of the distance it takes to
reach that node from the starting node.
Once you have explored all the adjacent nodes, you
can backtrack and pick the node which has the
shortest distance to reach the destination node.
Given <graph description >. Q: Give the shortest pathfrom node 0 to node 4.
Compared with other advanced prompts and in-context
learning techniques, the two proposed prompts perform better
on graph tasks. Based on the experiments, LLMs indeed pos-
sess preliminary graph reasoning abilities. Also, the benefits
of advanced prompting and in-context learning diminish in
complex graph problems and may even have a negative impact.
LLMs are also susceptible to false correlations, performing
poorly on graph structures such as chains and cliques.
To explore whether LLMs can truly comprehend graph
structures and reason on graphs, meanwhile, enhance the
performance of LLM-GQP tasks, [26] and [24] test LLMs
also using manual prompts, where [26] explores the conditions
under which LLMs can benefit from the inherent structural
information in the data and examines two potential factors in-
fluencing LLM’s performance: data leakage and homogeneity.
In summary, the conclusions are as follows:
•No evidence suggests that LLM’s performance is signif-
icantly attributed to data leakage.
•The performance of LLMs on target nodes is positively
correlated with the local homogeneity of the nodes.
[24] investigates the graph reasoning capabilities of LLMs
and introduces new evaluation metrics—comprehension, cor-
rectness, fidelity, and rectification—to assess LLMs’ pro-
ficiency in understanding graph structures and performing
reasoning tasks. The findings reveal that LLMs can effectively
understand graph structures and perform reasoning tasks.
However, LLMs still face challenges in structural reasoning,
particularly in multi-answer tasks where GPT models demon-
strate errors and overconfidence. In contrast, GPT-4 displays
improved self-correction abilities.
Beyond static graphs, LLMs’ ability to understand dynamic
graph structures is also assessed. Dynamic graphs change
over time, capturing temporal network evolution patterns.
LLM4DyG [25] introduces the LLM4DyG benchmark, which
uses prompting methods to evaluate LLMs’ spatio-temporal
understanding capabilities on dynamic graphs.
Prompt III-3: DST2. The newly proposed Disentangled
Spatial-Temporal Thoughts (DST2) prompting technique en-
hances LLMs’ spatial and temporal understanding of dynamic
graphs. DST2 is shown below:
Prompt III-3: DST2
DyG Instruction: In an undirected dynamic graph, (u,
v, t) means that node u and node v are linked with an
undirected edge at time t.
Task Instruction: Your task is to answer when two
nodes are first connected in the dynamic graph. Two
nodes are connected if there exists a path between them.
Answer Instruction: Give the answer as an integer
number at the last of your response after ’Answer:’Exemplar: Here is an example: Question: Given an
undirected dynamic graph with the edges [(0, 1, 0), (1,
2, 1), (0, 2, 2)]. When are node 0 and node 2 first
connected? Answer:1
Question: Question: Given an undirected dynamic
graph with the edges [(0, 9, 0), (1, 9, 0), (2, 5, 0), (1, 2,
1), (2, 6, 1), (3, 7, 1), (4, 5, 2), (4, 7, 2), (7, 8, 2), (0, 1,
3), (1, 6, 3), (5, 6, 3), (0, 4, 4), (3, 4, 4), (3, 6, 4), (4, 6,
4), (4, 9, 4), (6, 7, 4)]. When are node 2 and node 1
first connected?
Results show that LLMs have preliminary spatio-temporal
understanding capabilities on dynamic graphs. Dynamic graph
tasks become increasingly challenging with larger graph sizes
and densities while insensitive to periods and data generation
mechanisms.
We provide manual prompt examples for various graph
structure understanding tasks in Table I and Table II. Addi-
tionally, we test LLMs with GPT 3.5 for path, max flow, and
bipartite graph matching using manual prompts, as shown in
Figure 3, Figure 4 and Figure 5 respectively.
For self-prompting. Self-prompting refers to the process
where an LLM continuously updates the initial prompt to make
it easier for LLMs to understand and more beneficial for solv-
ing tasks. In other words, the LLM designs prompts based on
the original prompt. GPT4Graph [23] utilizes self-prompting
by continuously updating the prompt with descriptions related
to the graph. Specifically, first, the graph data is converted into
graph description languages, as shown in Section II-D. Then,
together with queries, it is inputted into the prompt handler to
create a prompt, which is then inputted into the LLM. Based
on the output of the LLM, the prompt is updated and re-
input into the LLM, repeating multiple rounds of updates to
obtain an optimized graph description context, such as context
summarization and format explanation. This process can be
seen as the LLM’s self-updating prompt procedure. Finally,
the optimized graph description context is input along with
the original input into the LLM to obtain the final result.
Prompt III-4: Self-prompting. The input original prompt is
shown below:
Prompt III-4: Self-prompting
Instructor: You are a brilliant graph master that can
handle anything related to graphs like retrieval, detection
and classification.
Graph description language: GML, GraphML as
shown in Section II-D.
Context: Node P357 has 4 neighbors, where each of
which are about anomaly detection with statistical
models...
Query: What is the clustering coefficient of node P357?
This paper conducts experiments on the obgn-arxiv [56] andAminer [57] datasets and finds that:
•The design of prompts significantly impacts the results.
The choice of graph description language, the orga-
nization of input data, and the position of in-context
knowledge, such as questions, statements, and examples,
all affect the model’s ability to understand the graph
structure.
•Role prompting techniques can improve the effectiveness
of LLMs by guiding the model to view the graph as
roles and relationships between roles in a specific context.
Providing LLMs with more semantic information leads to
more accurate results.
•Examples in prompts have mixed impacts on graph
structure understanding. Adding examples in prompts to
guide LLMs in understanding graph structures may not
necessarily improve the results; in some graph structure
learning tasks, examples may introduce noise.
API call prompts LLMs exhibit limited ability to perform
precise mathematical calculations, multi-step logical reason-
ing, spatial topological structuring, and temporal information
processing. To bridge these gaps, taking inspiration from
recent models such as ChatGPT and Toolformer [58], Graph-
ToolFormer [59] is proposed to equip LLMs with graph
reasoning capabilities by training them over a prompt dataset
that contains graph reasoning API annotated by ChatGPT.
These graph reasoning APIs are used to call external reasoning
tools. Then, the trained LLMs can solve graph tasks, from
loading graph data and inferring graph attributes to graph
partition tasks.
The framework consists of three parts. First, it generates a
prompt dataset by providing ChatGPT with a regular prompt,
guiding ChatGPT to add an API call to the original prompt,
and then creating a prompt with an API call.
Prompt III-5: API call prompts
Prompt III-5: API call prompts
Example 1
Input:(Regular prompt)
The structure of the benzene ring molecular graph of
benzene ring contains a hexagon.
Output:(API call prompt)
The structure of the [GL(”benzenering”)] molecular
graph of benzene ring contains a hexagon.
Example 2
Input:(Regular prompt)
What is the diameter of the binomial tree?
Output:(API call prompt)
The diameter of the binomial tree is [GR(GL(”gpr”,
”binomial tree”), ”toolx:diameter”) →r].
Second, fine-tune existing LLMs such as GPT-J [60] [61],
LLaMA [5] [62], etc., using technologies like LoRA [63]
on the generated prompt dataset. Thirdly, utilize the fine-
tuned LLM for inference to add graph reasoning API calls
………………Instructions: How many C-C-O triangles are in the molecule?Graph-enhanced prefix:Structural and textual featuresLLMResponse: There is 1 C-C-O triangle in the molecule.Fig. 7: Supervised fine-tuning (SFT) method in graph structure
understanding tasks. Prefix tuning is shown above: combine
graph structural and textual information as prefixes in prefix
tuning and input it into LLM with instructions, like GraphLLM
[64]. Instruction tuning can also be used.
into statements. After generating API call statements, how
can external graph tools be invoked? Graph reasoning query
processing comes in. Graph reasoning query processing entails
utilizing external graph reasoning tools based on API call
statements to obtain the final answer.
2)Supervised fine-tuning (SFT) method :Beyond lever-
aging prompts for graph-structured tasks with LLMs, cer-
tain studies have also implemented supervised fine-tuning of
LLMs, illustrated in Figure 7. GraphLLM [64] is committed
to addressing the obstacles in graph reasoning by LLMs and
introduces a hybrid model that inherits the capabilities of both
graph learning models and LLMs, enabling LLMs to interpret
and reason about graph data proficiently, utilizing the superior
expressive power of graph learning models.
C. Comparisons and Discussions
In the following part, we compare the prompting and SFT
methods mentioned above.
The prompting method can be divided into three cate-
gories: manual prompts, self-prompting, and API call prompts.
Most current methods primarily rely on manual prompts,
incorporating techniques like Chain of Thought (CoT) [65],
self-consistency [66], and in-context learning [67]. To obtain
better prompt representations, self-prompting methods are also
widely used. However, the exclusive use of manual prompts
and self-prompting offers limited enhancement to model per-
formance, as they merely tap into the pre-existing capabilities
of LLMs. Additionally, due to the limited input window of
LLM, the graph size that can be input to LLM at once is also
restricted, while graph sizes in the real world are typically
large.
For the prompting method, we also propose two feasible
directions to better leverage existing LLMs for handling struc-
ture understanding tasks. The first direction is breaking down
complex tasks into several sub-problems. While LLMs can
tackle simple graph tasks, they struggle with more challenging
ones. Breaking down complex graph understanding tasks into
simpler components enables LLMs to engage in multi-step
reasoning processes, leading to the resolution of complexGraph Learning Tasks
USKGQA
Given <knowledge graph>, the director who directs Inception also direct what?InceptionNolanOppenheimerLeonardois starred byGQL Generation
Given <graph>, the director who directs Inception also direct what? Use Cypher to answer.14320Node ClassificationGiven <graph>, which arxiv CS subcategory does paper ”paper title” with abstract ”paper abstract” belongs to? use the abbreviation to answer.Abstract: Text in curve orientation, despite being one of the common…Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.CCCCCOHCGiven <graph>, is this molecule active with H3C4?Graph Classification
14320Node Feature ExplanationGiven <graph>, which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS subcategories as a comma-separated list ordered from most to least likely, in the form ”cs.XX”, and provide your reasoning. Abstract: Text in curve orientation, despite being one of the common…Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.14320Edge ClassificationLearnable prompt
USInceptionNolanOppenheimerLeonardois starred by(a) (b) (c) 
(d) (e) (f) Fig. 8: Graph Learning tasks.
issues, such as GoT [59], which can help address more
intricate graph tasks like generating GNN frameworks, k-truss
tasks, kd-core tasks, etc. The second direction is API call
prompts. Inspired by ToolFormer [58], LLMs can be trained
as agents to utilize tools for graph tasks that are hard to
solve. However, current API call prompt methods [59] utilize
LLMs not as agents but solely to convert user queries into
API command strings for processing by subsequent programs,
exemplified in Prompt III-5 .
However, compared to prompting methods, fine-tuning
LLMs with graph data seems a better way to enhance their
understanding of graph structures. There are two mainstream
methods for fine-tuning LLMs: Supervised Fine-Tuning (SFT)
and Reinforcement Learning with Human Feedback (RLHF)
[6]. SFT helps LLMs understand prompts and generate mean-
ingful responses. However, SFT only offers a single human-
written response for each prompt, whereas RLHF provides
detailed human feedback through pairwise comparison label-
ing. Furthermore, to address the instability issue in PPO [68]
training, the Reward Ranked Fine-Tuning (RAFT) [69] can
also be attempted which requires online interaction. For offline
algorithms, methods like DPO [3] and Preference Ranking
Optimization (PRO) [70] can also be utilized for training
LLMs.
IV. G RAPH LEARNING TASKS
A. Tasks Introduction
Recently, LLMs have been shown to possess extensive
common sense and powerful semantic understanding capa-
bilities, fundamentally transforming the existing workflow
for processing text. However, whether LLMs can effectively
handle graph learning tasks, transferring their generalization
ability from text tasks to graph learning tasks, such as node
and graph classification, is still a research subject that needsexploring. These tasks require the model to learn and solve
graph learning tasks, as shown in Figure 8. In this section, we
present seven graph learning tasks along with their definitions.
Next, we introduce graph learning methods, categorized into
three types based on the role of LLMs: LLMs act as enhancers,
LLMs act as predictors, and graph prompts.
1) Node classification: The node classification task requires
LLM to learn based on the neighbors of a node or the attributes
of a node. It involves classifying unseen nodes in a given
graph, such as categorizing papers in an academic network
into different research directions, as shown in Figure 8 (a).
2) Graph classification: The graph classification task re-
quires LLM to classify the entire graph. LLM is given several
labeled graphs and is expected to classify unseen graphs. For
example, a molecule can be viewed as a graph, and LLM
can predict the properties or functions of the molecule by
classifying the graph, as shown in Figure 8 (b).
3) Edge classification: The edge classification task involves
classifying the edges in a graph. Existing methods improve
edge classification by training a learnable graph prompt and
combining it with a GNN or LLM, as shown in Figure 8 (c).
4) Node generation: The node generation task refers to pro-
viding requirements for an LLM to generate nodes, allowing it
to generate node attributes, which are then added to the TAG
to enhance it.
5) Knowledge graph question qnswering (KGQA): Knowl-
edge graph organizes data into a structured format, represent-
ing entities, properties, and relationships. Knowledge graph
question answering (KGQA) aims to capture the most appro-
priate answers by querying the knowledge graph (KG) using
natural language questions. This task evaluates the ability of
LLM to reason and understand the underlying graph structure
to provide accurate answers, as shown in Figure 8 (d).
6) Graph query language (GQL) generation: The graph
query language generation task involves generating graphTask Prompts
KGQA Given [knowledge graph], the director who directs Inception also direct what?
GQL Generation Given [graph], the director who directs Inception also direct what? Use Cypher to answer.
Node Classification Which arxiv CS subcategory does paper ”paper title” with abstract ”paper abstract” belongs to? use the abbreviation
to answer.
Graph Classification Given [graph]. Is this molecule active with H3C4?
Node Feature Explanation Abstract: Text in curve orientation, despite being one of the common text orientations in real world environment... Title:
Total Text A Comprehensive Dataset For Scene Text Detection And Recognition. Question: Which arXiv CS sub-category
does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least
likely, in the form ”cs.XX”, and provide your reasoning.
Edge classification learnable prompt
TABLE III: Prompts for Graph Learning Tasks, where [·] is the input of the data.
query languages, including GQL and Cypher, to perform op-
erations on graph databases. Evaluating LLM’s ability to gen-
erate GQL helps users extract information from the database,
as shown in Figure 8 (e).
7) Node feature explanation: Node feature explanation task
involves extracting the attributes of nodes in a text attribute
graph. For example, in an academic paper network, the node
attributes may include abstracts, titles, etc. LLM is expected to
provide reasoning for the classification process of nodes based
on their text attributes and explain the features of the nodes,
as shown in Figure 8 (f).
B. Graph Learning Methods
LLM-GIL studies focusing on graph learning tasks can be
categorized into three main groups: LLMs act as enhancers,
LLMs act as predictors, and graph prompts. When LLMs act
as enhancers, they leverage their advanced semantic under-
standing of the text, strong reasoning capabilities, and vast
knowledge repository to enhance the text attributes associated
with nodes in the graph to enhance GNNs. When LLMs act
as predictors, LLMs are queried or fine-tuned to predict task
results. Inspired by NLP ideas, the Graph prompt aims to
create a unified framework capable of solving multiple graph
learning tasks. Although LLMs are not used, the concept aligns
with LLM-based pipelines.
In summary, integrating LLMs in graph learning tasks
presents a promising avenue for advancing the field. By
leveraging the strengths of LLMs as enhancers and predictors,
along with the strategic use of graph prompts, researchers can
explore new directions for enhanced performance and more
profound insights in LLM-GIL tasks.
1)LLMs act as enhancers : LLMs act as enhancers per-
tains to the LLMs-GNNs pipelines, where LLMs assume an
enhancer role. Within this framework, LLMs are tasked with
processing text attributes, while GNNs are responsible for
handling graph structures, capitalizing on the complementary
strengths of both components to address graph learning tasks
effectively. LLMs bolster GNNs through three distinct mecha-
nisms: encoding the graph into embeddings (as shown in Fig-
Encoding graph into embeddings.Generating graph pseudo labels.Providing external knowledge/explanations.…Trainable LLM………Frozen LLM………Trainable LM……
TextEmbeddingsNode Text Attribute
GraphStructureGNN+Fig. 9: Encoding graph into embeddings, when LLMs act as
enhancers. Input the node text attribute into LM/LLM to obtain
text embeddings, then combine the text embeddings with the
graph structure for training and learning in GNNs.
ure 9), generating graph pseudo labels (as shown in Figure 10),
and providing external knowledge or explanations (as shown
in Figure 11). Subsequently, we will provide a comprehensive
elaboration on these three enhancement strategies.
Encoding graph into embeddings. LLMs possess signif-
icant semantic comprehension capabilities to encode better
node embeddings, as shown in Figure 9. TAPE [30] integrates
LM with LLM to generate node embeddings. The process
involves fine-tuning two LM models using original node text
attributes and LLM explanations for node prediction. TheEncoding graph into embeddings.Generating graph pseudo labels.Providing external knowledge/explanations.Unlabelednodes
NodeswithpseudolabelsTrainingGNNAnnotationwithLLMFig. 10: Generating graph pseudo labels, when LLMs act as
enhancers. Input unlabeled nodes into LLM for labeling, then
use the labeled nodes with pseudo-labels as input for training
the GNNs for graph learning.
Encoding graph into embeddings.Generating graph pseudo labels.Providing external knowledge/explanations.Node Text Attribute
LLMsEnhancedtextattributesNode Text Attribute
Designedqueries
LLMsExplanationforreasoningprocess1.2.
Fig. 11: Providing external knowledge/explanations, when
LLMs act as enhancers. Two pipelines are shown above. In
the first pipeline, input node text attributes into LLM for
elaboration, enhancing the detail of the text attributes. In the
second pipeline, input node text attributes and designed queries
into LLM. LLM leverages the text attributes to answer queries
and explains the reasoning process.
resulting embeddings are then used as input to train a GNN
model for node classification tasks. To unify graph data and
graph learning tasks, OFA [32] introduces a comprehensive
framework that unifies diverse graph data by describing nodes
and edges using natural language and encoding varied and
potentially cross-domain text attributes into feature vectors
within the same embedding space. The obtained feature vec-
tors are then fed into a GNN to tackle various downstream
tasks effectively. Moreover, SIMTEG [71] and GLEM [31]
involve training an LM with Lora and subsequently generating
embeddings as text representations, then a GNN is trained on
top of these text embeddings. On this basis, G-prompt [33]
introduces a graph adapter to extract node features, thereby
obtaining improved node representations.
Generating graph pseudo labels. Many existing pipelines
utilize LLMs to process text attributes as node features, then
feed the embeddings produced by LLM into a GNN model forlearning, as shown in Figure 10. However, the simultaneous
training of LLM and GNN poses a significant computational
challenge. To bridge this gap, GLEM [31] suggests training
the GNN and LM separately in a variational Expectation-
Maximization (EM) framework. In the E-step, the LM predicts
both gold labels and pseudo-labels from the GNN, while in the
M-step, the GNN predicts gold labels and LM-inferred pseudo
labels using the embeddings and pseudo-labels provided by the
LM.
Moreover, due to the high cost of annotation and the
necessity for GNN to learn from a substantial amount of
high-quality labeled data to ensure its performance on graph
tasks, leveraging the zero-shot learning capability of LLM
becomes advantageous. Therefore, employing LLM for graph
annotation can enhance GNN training even with limited la-
beled data. LLM-GNN [72] proposes to select a candidate
node set to be annotated. Subsequently, LLMs annotate the
candidate node set, and post-filtering is conducted to eliminate
low-quality annotations. Finally, the GNN is trained using
the high-quality annotation set and utilized for prediction.
LLM-GNN [72] proposes to select a candidate node set for
annotation by LLMs, followed by post-filtering to remove low-
quality annotations. Then, GNN is trained using high-quality
annotations for prediction.
Providing external knowledge/explanations. LLMs pos-
sess a vast knowledge base, enabling them to provide external
knowledge or explanations related to node features when en-
coding them, as shown in Figure 11. The additional knowledge
assists the model in better extracting and capturing node
features. Graph-LLM [73] utilizes LLMs, such as ChatGPT, to
explain text attributes, enhancing them and generating pseudo
labels. These enhanced attributes are then fed into a trainable
LLM, like Llama, to produce node feature embeddings. The
combined pseudo labels and embeddings are input into a GNN,
which delivers the final prediction outcomes.
Similarly, TAPE [30] leverages LLMs to provide external
explanations. In a citation network where each node contains
text attributes like title and abstract, the text attribute of each
node serves as input to an LLM. The LLM categorizes the
nodes and generates multiple predictions ranked in a list with
accompanying reasoning explanations. This approach aims
to extract the LLM’s reasoning capabilities while integrating
external knowledge to aid in understanding node text attributes
and extracting node features.
2)LLMs act as predictors. :When LLMs are predictors,
they are usually directly employed as standalone predictors.
The critical aspect of integrating LLMs as predictors lies
in crafting a well-designed prompt that encompasses text
attributes and graph structures, enabling LLMs to compre-
hend the graph structure effectively and enhance prediction
accuracy. Additionally, there are other methodologies to fine-
tune LLMs, such as utilizing techniques like LoRA [63] and
instruction tuning, aiming to deepen the LLM’s understanding
of the graph structure. Based on whether LLMs undergo
parameter training, they are categorized into prompting LLMs
and SFT LLMs, as shown in Figure 12.Prompting LLMs
Supervised fine-tuning (SFT) LLMs.………………Trainable LLMInstructions: How many C-C-O triangles are in the molecule?Response: There is 1 C-C-O triangle in the molecule.Response: There is no C-C-O triangle in the molecule.Response: There is 4C-C-O triangle in the molecule.Instructiontuning………………Frozen LLMManualprompt:The title of one paper is <Title>and its abstract is <Abstract>. This paper is cited by the following papers: <Titlelist1>. Each of these papers belongs to one category in: <Categories>.You need to analyze the paper’s topic based on the given title and abstract.Fig. 12: LLMs act as predictors. For prompting LLMs, input
designed manual prompts into LLM, enabling it to predict
nodes/links/graphs. For SFT LLMs, input instructions into the
LLM to generate multiple answers. Tuning the LLM is then
based on these multiple responses.
Prompting LLMs. The prompting method can be divided
into two categories. One type is the manual prompts, which
are manually written prompts.
Prompt IV-1: Manual Prompt Template with Slots. For
instance, Beyond Text [74], ENG [75], and Graph Agent
[76] provide a manual prompt template with slots. By filling
these slots with different examples, various prompts can be
constructed. For example:
Prompt IV-1: Manual Prompt Template with Slots
The title of one paper is <Title>and its abstract is
<Abstract >. This paper is cited by the following
papers: <Titlelist1 >. Each of these papers belongs to
one category in: <Categories >. You need to 1.Analyse
the papers’ topic based on the given title and abstract;
2.Analyse the pattern of citation information based on
their titles, and retrieve the citation information you
think is important to help you determine the category of
the first given paper. Now you need to combine the
information from 1 and 2 to predict the category of the
first given paper. You should only output one category.
Compared to manual prompts, LPNL [77] generates
Fig. 13: Examples for Node Classification Task with GPT4 -
Graph Learning Tasks.
prompts through sampling. Specifically, it conducts a two-
stage sampling process on the source node and each candidate
neighbor from the original candidate set to acquire anchor
nodes. Prompt generation is then based on these anchor nodes.
We provide manual prompt examples for various graph
learning tasks in Table III. Additionally, we test LLMs with
GPT 3.5 for node classification and KGQA using manual
prompts, as shown in Figure 13 and Figure 14.
Supervised fine-tuning (SFT) LLMs. IntructGLM [78] and
GraphGPT [79] both employ SFT to train LLM for the node
classification task. IntructGLM [78] utilizes a single LLM by
prompting methods. The prompt includes the description of
node attributes and structure through text descriptions and
corresponding queries. LLMs are then tasked with answer-
ing questions and determining node categories, leading to
fine-tuning through supervised learning. On the other hand,
GraphGPT [79] feeds graph structural information and textFig. 14: Examples for KGQA with GPT3.5 - Graph Learning
Tasks.
PrefixtasksDownstreamtasksUnifiedtasksGNNTrainingonunifiedtasks
TunablepromptPre-trainedGNNTrainingonPrefixtasksDownstreamtasksUnifying+DownstreamtasksTuningpromptfor
Fig. 15: Graph prompt for graph learning.Graph prompt meth-
ods first unify prefix and downstream tasks, then pre-train
GNN on the unified tasks. The pre-trained GNN, when faced
with different downstream tasks, combines with a tunable
prompt through tuning prompts to handle the downstream
tasks better.
into LLM via embedding. Subsequently, two rounds of in-
struction tuning are conducted to refine LLM and effectively
address the node classification task. IntructGLM [78] em-
ploys prompts to input subgraph structures into LLM, while
GraphGPT [79] inputs them into LLM through embedding.
3)Graph prompt :In graph learning tasks, a wide array of
tasks at the node, edge, and graph levels creates a challenge in
achieving compatibility between pre-training and downstream
tasks, potentially leading to negative transfer effects that can
harm the performance of downstream tasks and compromise
the reliability of transfer learning in graph data. Current
methods aim to harmonize pre-training and downstream tasks
to facilitate more effective transfer learning of graph infor-
mation. Despite these efforts, it remains essential to identify
task-specific differences for optimal performance. Inspired by
NLP, researchers have started incorporating prompts in graph
contexts to enable the reuse of pre-trained models across
various downstream tasks without the need for repeated fine-tuning, as shown in Figure 15. The integration of prompts
is crucial in assisting downstream tasks in achieving task-
specific optimal outcomes, bridging the gap between pre-
trained models and the diverse array of graph tasks to enhance
performance and transferability.
GPPT [80] and GraphPrompt [81] aim to unify pre-training
and downstream tasks in graph learning. GPPT transforms
node classification tasks into edge prediction tasks and em-
ploys masked edge prediction for GNN pre-training. Mean-
while, GraphPrompt combines node and graph classification
tasks into a subgraph similarity prediction task and utilizes
graph prompt functions, introducing unified instances and task
templates to enhance performance. Subsequent research, like
All in One [82], further consolidates edge, node, and graph
classification tasks into a single framework using multi-task
prompting approaches, standardizing graph prompts similar to
language prompts and enhancing initialization through meta-
learning techniques for improved reliability and generality
across different tasks in graph data analysis.
C. Comparisons and Discussions
For addressing graph learning tasks, existing methods [30]
[79] [82] categorize based on the role of LLM into three types:
LLMs act as enhancers (LLM-GNN pipelines), LLMs act as
predictors (LLM pipelines), and graph prompts. In the part
of graph prompts, we introduce the prompting engineering
in GNNs without utilizing LLMs. Graph prompts aim to
unify downstream tasks and construct a universal framework.
Therefore, it is compared with LLM-GNN pipelines and LLM
pipelines to provide a comprehensive overview.
When LLMs act as enhancers, the most popular pipeline is
the LLM-GNN pipeline. There are three categories of LLM-
GNN pipelines, depending on how LLM enhances GNN:
encoding the graph into embeddings, generating graph pseudo
labels, and providing external knowledge/explanations. How-
ever, the LLM-GNN pipelines that are currently available are
not end-to-end pipelines, meaning that LLM and GNN cannot
be trained together. LLM and GNN can be trained separately
using frameworks like EM framework [31] or by freezing
LLM and using it as an external knowledge base. Co-training
LLM and GNN can lead to issues like gradient vanishing,
which is a significant obstacle in current LLM-GNN pipelines
due to the large number of parameters in LLM compared
to GNN. To solve this problem, methods like knowledge
distillation can reduce the number of LLM parameters while
retaining the beneficial capabilities for downstream tasks.
When LLMs act as predictors, two main methods are
used: prompting LLMs and SFT LLMs. All approaches for
fine-tuning LLMs can be reviewed in the ”comparisons and
discussions” section of Section III. Currently, SFT and DPO
are popular methods for fine-tuning LLMs.
For graph prompt, the workflow involves unifying pre-
training and downstream tasks, followed by prompt tuning
for different downstream tasks through prompt engineering,
as shown in Figure 15. Graph prompts require fewer tunableGraph ReasoningSortingSort the following list of numbers in ascending order. Output only the sorted list of numbers, no additional text. Input: [5, 1, 0, 1, 2, 0, 4, 8, 1, 9, 5, 1, 3, 3, 9, 7] Output: [0, 0, 1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 7, 8, 9, 9]Find the intersection of two sets of numbers. Output only the set of numbers that are present in both sets, no additional text. Input Set 1: [13, 16, 30, 6, 21, 7, 31, 15, 11, 1, 24, 10, 9, 3, 20, 8] Input Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 20, 17, 19, 26, 23]Set Operations
Count the frequency of how many times each country is explicitly named in the input text. You can generate any intermediate lists and states, but the final output should only contain the frequency of each country that appears at least once in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each country in the output as in the input text): {{ ”country1”: frequency1, ”country2”: frequency2, ... }}Keyword CountingKeyword: frequencyMerge the following 4 NDA documents -into a single NDA, maximizing retained information and minimizing redundancy. Output only the created NDA between the tags and , without any additional text. Here are NDAs: [four documents] Document MergingMath word problemsJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?Math problems
Multi-hop Question AnsweringQuestion triplets: (’Hypocrite’, directed by, $1), ($1, death date, $2) Question: When did the director of film Hypocrite (Film) die? To answer this question, we answer the following subquestions: (1) Who directed Hypocrite (Film)? (2) When did Miguel Morayta die? Logic reasoning• Premises: 1.It is not true that some giant language models do not have good performance. 2.All language models with good performance are used by some researchers. 3.If a language model is used by some researchers, it is popular. 4.If BERT is a giant language model, then GPT-3 is also a giant language model. 5.BERT is a giant language model. • Hypothesis: GPT-3 is popular. Give hypothesis label, true or false.51012048195133970011112334557899(a) (b) (c) (d) 
(e) (a) 
(f) (g) Fig. 16: Graph-formed Reasoning Tasks.
Fig. 17: Illustration of human logical derivation. [35]
parameters compared to LLM-GNN and LLM pipelines; how-
ever, they have a shallower semantic understanding of graph
attributes. In LLM pipelines, LLMs need to undergo alignment
tuning before they can be used for various downstream tasks.
In LLM-GNN pipelines, there is a general trend of training
GNNs. Combining LLM-GNN and graph prompts is possible
because graph prompts are designed for GNNs through prompt
engineering and can be applied to LLM-GNN pipelines. By
leveraging LLM’s robust semantic representation capabilities
and the lightweight fine-tuning of graph prompts, similar
results can be achieved.
Classical graph tasks, such as node classification on at-
tributed static networks, have recently obtained the most
attention. However, there is potential for more complex tasks
in the future, such as predicting graph evolution on dynamic
graphs. Leveraging LLM models that are suitable for handling
sequential data and can process time series data, along with
GNNs that are adept at capturing changes in graph structures,
can help address a broader range of problems effectively. By
combining the strengths of LLM and GNN, we can tackle
more challenging tasks in the field of graph analysis.V. G RAPH -FORMED REASONING
A. Tasks Introduction
Graph-formed reasoning refers to combining the graph form
with LLMs to obtain more accurate and reliable answers.
LLMs have strong reasoning capabilities, and many prompting
methods are proposed to enhance LLMs’ reasoning abilities,
addressing algorithmic problems, mathematical issues, etc.,
such as chain of thought, self-consistency, in-context learning,
and more. However, these methods diverge from the patterns
of human thought. The human thought process is typically
non-linear rather than a simple chain of continuous thoughts,
like in Figure 17. Graphs can represent the thinking patterns
of individuals during the thought process. Suppose LLMs
can also use graph-formed reasoning for inference. In that
case, they may be able to solve more complex problems,
such as algorithmic problems, logical reasoning problems,
and mathematical word problems, as shown in Figure 16. In
this section, we present seven graph-formed reasoning tasks
along with their definitions. Next, we introduce graph-formed
reasoning methods involving two types of reasoning: think on
the graph and verify on the graph.
1) Sorting: The problem of sorting involves arranging
certain elements in a specific order. For example, sorting a
list of duplicate numbers from 0 to 9 can be done using a
merge-based sorting algorithm. First, the input sequence of
numbers is divided into subarrays. Then, these subarrays are
sorted individually and merged to form the final solution, as
shown in Figure 16 (a).
2) Set operations: Set operation task mainly focuses on
set intersection. Specifically, the second input set is split into
subsets and the intersection of those subsets with the first input
set is determined with the help of the LLM, as shown in Figure
16 (b).
3) Keyword counting: The keyword counting task aims to
determine the frequency of specific keywords within a given
category in the input text. The input text is divided intoThinkongraphVerifyongraphLLM's intermediate answerLLM’sfinalanswerInput
Deletedintermediate answer
ThinkingprocessInputLLM's intermediate conclusionVerificationVerify whether two conclusions from two paths are the same.VerifyingprocessFig. 18: Graph-formed reasoning. Two directions: think on graphs and verify on graphs. Think on the graph refers to using
the graph structure to derive the final conclusion during the LLMs’ reasoning process. Verify on the graph refers to using the
graph to verify the correctness of the LLMs’ intermediate and final output.
multiple paragraphs, and the keywords are counted in each
paragraph, with the sub-results aggregated, as shown in Figure
16 (e).
4) Document merging: Document merging is the process
of generating a new document based on multiple input docu-
ments that have overlapping content sections. The goal is to
minimize duplication as much as possible while preserving the
maximum amount of information, as shown in Figure 16 (c).
5) Math word problems: Math word problems include
single- and multi-step word problems with addition, multipli-
cation, subtraction, division and other math topics. LLM re-
quires an understanding of text and mathematical relationships
and involves a multi-step reasoning process where calculations
are performed step by step to arrive at an answer ultimately,
as shown in Figure 16 (d).
6) Multi-hop question qnswering: Multi-hop question an-
swering requires LLM to retrieve and integrate information
from multiple text passages or multi-hop graphs to answer
questions. For a complex reasoning question, LLM uses a
sophisticated thinking process to perform reasoning and ul-
timately arrive at the correct answer, as shown in Figure 16
(f).
7) Logic reasoning: Logical reasoning is a process aimed at
concluding rigorously. It occurs in inference or argumentation,
starting from a set of premises and reasoning towards a
conclusion supported by those premises. Propositional logic
is the most fundamental logical system, consisting of p, q, r,
and various operations, as shown in Figure 16 (g).
B. Graph-formed Reasoning Methods
The graph form, with its inherent structural features, not
only mimics human reasoning patterns but also validates
answers from LLM through the relationships between nodes
and local structure. Existing work can roughly be divided
into two categories: think on the graph and verify on the
graph , as shown in Figure 18. Think on the graph refers
to LLM thinking in the form of a graph, where each node
on the graph represents a step in the thinking process oran intermediate conclusion during thinking, and the edges
on the graph indicate the direction of LLM inference or the
relationships between intermediate thinking steps. In this way,
the LLM thinking process can be visually represented in graph
form. Verify on the graph means verifying the consistency and
correctness of answers by utilizing the graph’s structure. For
example, if the end node of different paths is the same, the
results derived from different paths should be the same. If
contradictory conclusions arise, then the obtained conclusion
is incorrect.
1)Think on the graph :The GoT* reasoning method [36]
is proposed with a two-stage framework to enable LLM to
reason on a graph for answering multiple-choice questions.
Initially, the input query is converted into a graph form, and
with the incorporation of graph and multimodal features, LLM
generates rationale. This rationale updates the graph to a graph
with rationales, which is then combined with the original input
and fed into the decoder to obtain the final answer.
However, GoT* allows LLM to enhance the graph us-
ing multimodal information but does not reason step-by-
step deduction in graph form. The Graph of Thought (GoT)
[34] represents LLM’s intermediate thinking as an arbitrary
graph, facilitating powerful prompting for solving algorithmic
problems like sorting and keyword counts. LLM thoughts are
depicted as vertices in this approach, with edges representing
dependencies between them. By continuously adding LLM
responses to the graph, arbitrary thoughts can be aggregated,
forming a directed acyclic graph.
Multiple LLMs can also be collaboratively harnessed to
tackle complex mathematical challenges, extending beyond
the capabilities of a single LLM. Cumulative Reasoning (CR)
[35] is proposed as a more human-like reasoning process. CR
utilizes three LLMs in different roles: the proposer, verifier,
and reporter. The proposer suggests the next step, the verifier
checks the accuracy of the steps, and the reporter decides
when the reasoning process should end. Three roles of LLMs
collaborate to achieve more accurate reasoning processes.Task Prompts
Sorting <Instruction >Sort the following list of numbers in ascending order. Output only the sorted list of numbers, no
additional text. </Instruction ><Examples >like Input: [5, 1, 0, 1, 2, 0, 4, 8, 1, 9, 5, 1, 3, 3, 9, 7] Output: [0, 0,
1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 7, 8, 9, 9] </Examples >Input: [input list]
Set Operations <Instruction >Find the intersection of two sets of numbers. Output only the set of numbers that are present in
both sets, no additional text. </Instruction ><Examples >like Input Set 1: [13, 16, 30, 6, 21, 7, 31, 15, 11, 1, 24,
10, 9, 3, 20, 8] Input Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 20, 17, 19, 26, 23] Output: [24, 10, 20, 8]
</Examples >Input Set 1: set1 Input Set 2: set2
Keyword Counting <Instruction >Count the frequency of how many times each country is explicitly named in the input text. You can
generate any intermediate lists and states, but the final output should only contain the frequency of each country that
appears at least once in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling
for each country in the output as in the input text): {{”country1”: frequency1, ”country2”: frequency2, ... }}
</Instruction ><Approach >To count the frequency for each country follow these steps: 1. Split the input passage
into four paragraphs of similar length. 2. Count the frequency of each country in each paragraph. 3. Combine the
frequencies of each country from each paragraph by adding them together. </Approach ><Examples >(Omitted)
</Examples >Input: input text
Document Merging Merge the following 4 NDA documents <Doc1>-<Doc4>into a single NDA, maximizing retained information
and minimizing redundancy. Output only the created NDA between the tags <Merged >and</Merged >, without
any additional text. Here are NDAs: [four documents]
Math word problems Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends
every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
in dollars does she make every day at the farmers’ market?
Multi-hop Question Answering Question triplets: (’Hypocrite’, directed by, $1), ($1, death date, $2) Question: When did the director of film
Hypocrite (Film) die? To answer this question, we answer the following subquestions: (1) Who directed Hypocrite
(Film)? The film Hypocrite was directed by Miguel Morayta. (2) When did Miguel Morayta die? Miguel Morayta
died on 19 June 2013. So the answer is 19 June 2013.
Logic reasoning • Premises: 1. It is not true that some giant language models do not have good performance. 2. All language
models with good performance are used by some researchers. 3. If a language model is used by some researchers,
it is popular. 4. If BERT is a giant language model, then GPT-3 is also a giant language model. 5. BERT is a
giant language model. • Hypothesis: GPT-3 is popular. • Label: [True]
TABLE IV: Prompts for Graph-formed Reasoning.
2)Verify on the graph : Verify on the graph is to validate
the intermediate reasoning results of LLM to enhance its
performance. The Reasoning Graph Verifier (RGV) [83] in this
study assumes a logical connection between the intermediate
steps of different inference paths created by LLM. This allows
the multiple solutions generated by LLM for a reasoning task
to be structured into a reasoning graph, aiming to improve
the accuracy and reliability of the outcomes. By constructing
reasoning graphs from the various solutions provided by LLM,
a verifier is trained to determine the correctness of the resulting
reasoning graph. During the prediction phase, RGV assesses
the solutions and selects the highest-scoring one as the final
answer.
However, this work trains an extra model to determine
whether the graph formed by the solutions generated by LLM
is correct rather than utilizing the knowledge within the graph
and the relationships between the knowledge for validation.
The Graph-guided CoT [84] approach aims to improve the
relevance of rationales generated by CoT during multi-step
reasoning. It starts by extracting triplets from questions using
LLM to build a question graph and generates intermediate sub-
questions from this graph. To ensure the rationale from LLM is
logical, Retrieval Augmented Generation (RAG) is used. In an
open-book scenario, knowledge retrieval is based on the sub-questions, providing retrieved documents and sub-questions
as input to LLMs. LLMs generate rationales for the sub-
questions, creating a rationale graph. Based on the rationale
graph, the study assesses whether the generated rationales
aid in solving the original question. By iteratively generating
intermediate rationales, the solution to the original question
can be determined.
Finally, we provide manual prompt examples for various
graph learning tasks in Table IV. Additionally, we test LLMs
with GPT-4 for sorting and logic reasoning using manual
prompts, as shown in Figure 19.
C. Comparisons and Discussions
Graph-formed reasoning is categorized into think on the
graph andverify on the graph .Think on the graph refers to
using the graph structure to derive the final conclusion during
the reasoning process with LLM. On the other hand, verify
on the graph involves treating the intermediate or final results
generated by LLM as nodes on the graph and using the graph
to determine if there are contradictions between the nodes,
thus verifying the correctness of the LLM output.
For “think on the graph”, a common issue with existing
approaches is their lack of convenience. Compared to CoT
and SC, the reasoning processes in current works are complex,Fig. 19: Examples for Logic Reasoning Task with GPT4 -
Graph Reasoning Tasks.
requiring multiple stages of reasoning and validation. Graph
of thought methods are not plug and play, which contradicts
the original intent of prompts. Even though using more LLMs
can simplify the reasoning and validation process, it raises the
cost and barrier to entry for reasoning. Therefore, the current
challenge is to find a plug-and-play, low-barrier LLM graph
reasoning method that improves LLM reasoning capabilities.
For “verify on the graph”, the current approaches have yet to
utilize the nature of the graph structure for validation. Existing
methods either retrain a model to determine correctness or use
a KG for assessment without using the relationships between
nodes to infer whether the conclusions within each node in
the graph are correct.
Therefore, for the “think on the graph,” the future direction
could focus on developing a plug-and-play, low-barrier LLM
graph reasoning method that enhances LLM reasoning abili-
ties, a pressing issue that needs to be addressed. On the other
hand, concerning the “verify on the graph” method, future
research could explore how to utilize the relationships between
nodes in the graph structure to verify the outputs of LLM or
the reasoning process itself.VI. G RAPH REPRESENTATION
A. Tasks Introduction
LLMs’ powerful text representation abilities empower text
embeddings to capture deeper semantic nuances, which also
can enhance graph representations, particularly for Text At-
tributed Graphs (TAGs). When dealing with structured text
data, the key challenge is integrating graph structures into text
embeddings produced by LLMs to enhance their informative-
ness or enable LLMs to process text embeddings with graph
structures within the text space. Moreover, effectively incor-
porating the graph description within the prompt is essential
for LLMs, especially in closed-source models like ChatGPT,
where the embedding is invisible. How the graph is encoded
within the prompt influences the model’s comprehension of
the graph. Thus, we summarize three types of graph repre-
sentation: graph embedding ,graph-enhanced text embedding ,
andgraph-encoded prompts , as shown in Figure 20. Next, we
introduce graph-formed reasoning methods corresponding to
the above three types.
1) Graph embedding: Graph embedding focuses on trans-
forming a graph into a specific ordered sequence, which is then
fed into an LLM to learn the sequence’s embedding using their
excellent semantic capturing ability and then derive the graph
embedding.
2) Graph-enhanced text embedding: Graph-enhanced text
embedding emphasizes incorporating structural embedding
into text embedding. There are two types of embeddings:
structural embedding, which captures the local structure, and
text embedding, which captures the semantic meaning. How to
combine these two types of embeddings is the core of graph-
enhanced text embedding.
3) Graph-encoded prompts: Graph-encoded prompts con-
centrate on how to describe a graph so that LLMs can
understand it more efficiently and then input it into LLMs.
For instance, in a regular graph, the graph can be placed in a
story context by assuming that the relationships between the
nodes are friends or colleagues.
With the emergence of LLM, much work has been done on
graph representation. Three goals of the graph representation
direction can be identified from the above three categories:
to obtain better graph embeddings as an input into GNNs, to
obtain better text embeddings as an input into LLMs/LMs,
and to get better prompts for graph description as an input
into LLMs.
B. Graph Representation Methods
For the three categories of tasks mentioned above, each
type of task has specific focuses, technical characteristics, and
objectives.
1)Graph embedding :Text data is sequential, while graph
data is structural, posing a challenge for LLMs, which excel at
handling text but struggle with graphs. How do we transform
graphs into sequences? Graph embedding methods use specific
order sequences to represent the graph, where specific order
represents graph structure. WalkLM [38] aims to enhanceGraphrepresentationGraph embeddingGraph-enhanced text embeddingGraph-encoded promptsGraph1432014314013204……SpecificordersequencesLLM/PLMGraphEmbeddingsTextembeddingsTexttokensLLM/PLMTextembeddingswithgraphstructureGraphstructuralembeddings+……
Prompt:(placinggraphGinmultiplecontexts)GdescribesafriendshipgraphamongJames,David,John…Gdescribesaco-authorshipgraphamongJames,David,John…GdescribesasocialnetworkgraphamongJames,David,John………LLMResponse: ……Fig. 20: Graph representation. Three types of graph representation are shown: graph embedding, graph-enhanced text embedding,
and graph-encoded prompts. Graph embedding methods use specific order sequences to represent the graph. Graph-enhanced
text embedding emphasizes incorporating structural embedding into text embedding. Graph-encoded prompts concentrate on
how to describe a graph in prompts.
graph representations in TAGs by utilizing a language model.
Initially, text sequences are generated on the TAG through
the random walk algorithm, capturing structural features and
node proximity. By incorporating text information from nodes
and edges into these sequences based on the graph structure,
the texturing process preserves component attributes. Subse-
quently, these sequences are input into a masked language
model for training, where each token represents a node or
edge, leading to improved graph representations and enhanced
downstream task efficiency. Notably, various masked language
model options, including LLMs, are available.
While WalkLM [38] focuses on superior graph embeddings
for tasks like node classification, GraphText [37] transforms
graphs into the natural language to enable LLMs to process
graph data in the text domain, leveraging LLMs’ generalization
capabilities for graph tasks. GraphText [37] reformulates graph
reasoning as text-to-text problems, establishing text input and
output spaces. GraphText first constructs grammar trees for
graphs, then traverses them to generate graph text sequences,
and finally maps the graph to the text space. The text input is
then fed into an LLM, with the LLM results mapped to the
label space, effectively enabling LLMs to handle graph tasks.
2)Graph-enhanced text embedding :Current work focuses
on simply passing graph structure information to the LLM
through prompts without deeply learning the graph structure,
which can lead to an LLM’s insufficient understanding of
complex structural relationships.
DGTL [39] integrates graph information into text with
LLMs for node classification tasks. It begins by inputting text
into a frozen LLM to create text embeddings from the last
layer. Then, a disentangled graph learning method is employed
to extract various structural details and generate structure
embeddings. These structure embeddings are combined with
the text embeddings and fed back into the frozen LLM for
node classification. The entire process is fine-tuned to optimize
the disentangled graph learning for better results.
While DGTL [39] concentrates on utilizing LLMs to in-
tegrate text and graph structure for graph tasks, G2P2 [85]
emphasizes merging graph structure with text to address textclassification tasks. Textual data commonly exhibit network
structures, such as hyperlinks in citation networks or purchase
networks, which encapsulate meaningful semantic relation-
ships that can enhance text classification performance.
G2P2 [85] is proposed to tackle low-resource text clas-
sification through a dual approach. Three graph interaction-
based contrastive strategies are introduced during pre-training
to jointly pre-train the graph-text model. In the downstream
classification process, efforts are made to facilitate the joint
pre-trained model in achieving low-resource classification.
3)Graph-encoded prompts :The prompting method is
crucial for LLMs to solve tasks. For closed-source LLMs,
the prompt serves as instructions to guide the LLM in under-
standing and solving problems. Therefore, effectively encoding
graphs in the prompt is vital for LLMs to comprehend graph
structure and solve graph tasks. Graph encoding refers to how
graphs are represented in the prompt.
Talk Like A Graph [86] introduces diverse graph encoding
techniques by placing the same graph in multiple contexts.
This strategy highlights how a node, which may lack intrinsic
meaning, can be interpreted differently based on the context;
for instance, a node could represent a person named David,
with edges indicating various relationships like co-authorships
or friendships. When asking LLM the degree of one node, in
the given contexts, that equals how many friendships David
has.
In contrast, Talk Like A Graph [86] primarily emphasizes
text modality graph encoding, while Which Modality Should I
Use [87] employs three encoding modalities - text, image, and
motif - to encode graphs. The latter method utilizes different
prompt techniques to evaluate the overall connectivity of a
graph, enabling LLMs to handle intricate graph structures
more effectively. Specifically, the text modality encoding pro-
vides insights into subgraphs and their connections at a local
level, while the motif modality encoding captures essential
graph patterns like stars, triangles, and cliques, offering a bal-
anced perspective on local and global information. Moreover,
the image modality encoding delivers a broader view of nodes
with limited labels, effectively utilizing the input context.Query: What other works does the director who directed Inception have?
KGsLLMs1. "The Dark Knight Trilogy" 2. "Interstellar"3. "Dunkirk"4. "Memento"5. "The Prestige"6. "Insomnia"Incomplete answerQuery: What other works does the director who directed Inception have?LLMs1. "The Dark Knight Trilogy" 2. "Interstellar"3. "Dunkirk"4. "Memento"5. "The Prestige"6. "Insomnia"7. “Oppenheimer"Complete answer+KGs enhanced LLMsFig. 21: KG-based augmented retrieval. Knowledge graphs can
enhance LLMs to provide more comprehensive answers.
In comparing these two methods, Talk Like A Graph [86]
focuses on diverse graph encoding within text modality by
constructing contexts, whereas Which Modality Should I Use
[87] utilizes multiple modalities to encode graphs compre-
hensively, enhancing the LLMs’ ability to understand graph
structures.
C. Comparisons and Discussions
Graph embedding focuses on transforming a graph into a
specific ordered sequence, which is then fed into an LLM
to learn the sequence’s embedding and derive the graph em-
bedding. On the other hand, graph-enhanced text embedding
emphasizes incorporating structural embedding into text em-
bedding. Lastly, graph-encoded prompts concentrate on how
to describe a graph and input it into an LLM.
However, due to LLMs’ powerful text representation capa-
bilities, the first two methods exhibit a deep semantic under-
standing of graph attributes. However, they still need suitable
structural information capturing, which remains rudimentary
and inadequate. Additionally, aligning the graph structure
features with text features to better represent the graph’s
features is a current issue that needs to be addressed.
For graph-encoded prompts, most methods build a narrative
context for the graph or describe it multimodally before feed-
ing it into an LLM. Both methods enable the LLM to interpret
the graph from various perspectives to improve performance.
The critical challenge currently lies in designing diverse and
easily understandable graph descriptions for LLMs, convey-
ing essential graph descriptions while enhancing the LLM’s
comprehension of the input description.
VII. K NOWLEDGE GRAPH BASED AUGMENTED
RETRIEVAL
LLMs have shown remarkable reasoning capabilities in
challenging tasks, sparking debates on the potential replace-
ment of Knowledge Graphs (KGs) in triplet form (subject,
predicate, object) by LLMs. Recent LLMs are seen as viable
alternatives to structured knowledge repositories such as KGs,indicating a shift towards utilizing LLMs for processing real-
world factual knowledge [88] [89].
A. LLMs limitations and comparison with KGs
LLMs, while powerful, face several significant challenges:
•Hallucination is a common issue for LLMs due to a
lack of domain-specific knowledge and knowledge ob-
solescence, leading to incorrect reasoning and reduced
credibility in critical scenarios like medical diagnosis and
legal judgments [88] [90] [43]. Although some LLMs can
explain predictions through causal chains, they struggle
to address hallucination effectively. Integrating external
KGs can help mitigate these problems [41].
•Insufficient domain knowledge hampers LLM perfor-
mance in specific areas, including private datasets, ne-
cessitating the integration of domain-specific knowledge
graphs to enhance their ability to answer domain-specific
questions [40].
•LLMs struggle with recalling facts when generating
knowledge-based content, despite excelling in learning
language patterns and conversing with humans [89].
•LLMs have limitations in accurately capturing and re-
trieving foundational knowledge, hindering their ability
to access factual information effectively [42].
In contrast, KGs like Wikipedia and DBpedia are structured
repositories of rich factual knowledge, providing a more
explicit and reliable source of information compared to the
black-box nature of LLMs, as shown in Figure 21. How do
we measure the shortcomings of LLM relative to KG? KGLens
is proposed as an effective method to evaluate the factual
accuracy and identify knowledge gaps in LLMs by assessing
the alignment between a KG and LLM [91].
B. Solutions to LLMs limitations
To address the limitations of LLMs, such as hallucination,
insufficient domain knowledge, etc., integrating LLMs with
KGs is a potential way to allow LLMs to learn knowledge
from KGs and enhance their capabilities. The REASONING
ON GRAPHS (RoG) framework [43] synergizes LLMs with
KGs for faithful and interpretable reasoning. Specifically,
RoG utilizes a planning retrieval-reasoning framework where
relation paths grounded by KGs are generated as faithful plans.
These plans are then used to retrieve valid reasoning paths
from KGs to facilitate LLMs’ faithful reasoning. Existing work
has taken on the challenges posed by the four main limitations
of LLMs through distinct perspectives, each offering unique
solutions.
Addressing the first limitation concerning hallucination is-
sues in LLMs, the Head to Tail benchmark [88] is introduced
to assess LLMs’ reliability in answering factual questions
and to evaluate the probability of hallucination in generating
KG triples. Additionally, it explores whether factors like
model size or instruction tuning can enhance LLM knowledge.
Think-on-Graph (ToG) [41] partially addresses hallucination
by involving the LLM agent in iteratively searching KGs,
identifying promising reasoning paths, and providing likelyreasoning outcomes. The second limitation is LLM needs
domain-specific knowledge. To tackle this, GLaM [40] is
developed to convert knowledge graphs into text paired with
labeled questions and answers, allowing LLMs to acquire and
respond to domain-specific knowledge. Regarding the third
limitation related to LLMs forgetting facts, integrating KGs
with PLMs (KGPLMs) [89] is introduced to enhance the
model’s ability to recall facts compared to standalone LLMs.
This approach emphasizes the competitive and complementary
relationship between LLMs and KGs, where LLMs improve
knowledge extraction accuracy, and KGs guide LLM training
to enhance memory and knowledge application capabilities.
Finally, the fourth limitation pertains to LLMs’ challenge
in accurately retrieving and returning knowledge from KGs.
KGs can enhance LLM performance by incorporating them
during pre-training and inference stages or to deepen LLM’s
understanding of acquired knowledge. Graph Neural Prompt-
ing (GNP) [42] is proposed to augment pre-trained LLMs
using foundational knowledge, such as retrieval-augmented
generation, to facilitate effective learning from KGs. GNP [42]
retrieves and encodes relevant, grounded knowledge to gener-
ate Graph Neural Prompts, embedding vectors that provide
guidance and instructions for LLMs.
C. Other KG + LLMs works
1)KG tasks with LLMs :Moreover, LLMs can enhance
KGs to tackle a broader array of challenges. By leverag-
ing LLMs, KGs can be fortified to perform various KG-
related tasks such as embedding, completion, construction,
text generation from graphs, and question answering [90].
An illustrative example is how LLMs can support KG tasks
such as knowledge graph alignment. In entity alignment tasks
between different knowledge graphs, the objective is to iden-
tify pairs of entities representing the same entity. To address
this, AutoAlign [92] facilitates alignment without the need for
expensive manual seed creation. Specifically, AutoAlign [92]
automatically identifies similarities between predicates across
different KGs with the assistance of LLMs.
2)Applications of KGs + LLMs :The combination of KGs
and LLMs has other applications as well. For instance, it can
address tasks like multi-document question answering. Knowl-
edge Graph Prompting (KGP) [93] is introduced to design
appropriate context by building and exploring a knowledge
graph. Subsequently, this context guides LLMs for answering
multi-document questions.
D. Summary
In conjunction with LLMs, the future directions for KGs fo-
cus on overcoming challenges and seizing opportunities in this
evolving field. Firstly, leveraging KGs for Hallucination Detec-
tion in LLMs aims to address the issue of generating inaccurate
content. Secondly, utilizing KGs for Editing Knowledge in
LLMs will enable the swift adaptation of internal knowledge
to real-world changes. Moreover, the challenge of injecting
knowledge into Black-box LLMs due to restricted access to
internal structures necessitates innovative approaches. Lastly,
OccupationsLLM tuningBehavior GraphFig. 22: Graph-LLM-based applications - Recommendation
systems. This shows LLM for graph data understanding in
online job recommendations [46].
integrating Multi-Modal LLMs with KGs can enrich handling
diverse data types within knowledge graphs [90].
VIII. G RAPH -LLM- BASED APPLICATIONS
Graph-LLM-based applications refer to frameworks that
integrate graphs with LLMs. Apart from their applications
in graph-related tasks, they are also utilized in various other
domains (as shown in Figure ??), such as conversational
understanding and recommendation systems, as shown in
Figure 22. Common frameworks involve combining GNNs
with LLMs, merging graph data with LLMs, and exploring
additional innovative approaches that leverage the advantages
between graph structures and language models for diverse
applications.
1) Conversational understanding: By combining LLM
with graph traversal, collaborative query rewriting [94] is
proposed to improve the coverage of unseen interactions,
addressing the flawed queries users pose in dialogue systems.
Flawed queries often arise due to ambiguities or inaccura-
cies in automatic speech recognition and natural language
understanding. When integrated with graph traversal, LLM
can effectively navigate through the graph structure to retrieve
relevant information and provide more accurate responses.
2) Response forecasting: LLM can effectively handle social
networks and extract latent personas from users’ profiles and
historical posts. SOCIALSENSE [95] is proposed to utilize
LLMs to extract information to predict the reactions of news
media. By analyzing individuals’ characteristics and behavior
patterns within social networks, LLM can effectively predict
the impact of news releases and prevent unintended adverse
outcomes.
3) Multi-domain dialogue state tracking: LLM can learn
from multi-domain dialogue history, query, and graph prompts,
enabling it to track dialogue states and generate dialogue
content, like SHEGO [96]. By incorporating information from
various sources, such as previous dialogue exchanges, user
queries, and relevant graph prompts, LLM can understand the
conversation’s context and dynamics, allowing LLM to track
the current dialogue state effectively and generate appropriate
responses or dialogue content based on the inputs.
4) Recommendation systems: LLMs can also help address
issues in recommendation systems [46], as many tasks in
recommendation systems require learning graph structures,
such as user-item interaction networks. LLMRec [44] aimsto enhance recommendation systems by tackling data sparsity
by adopting three simple yet effective LLM-based graph-
enhancement strategies.
5) Graph neural architecture search: LLMs can help ad-
dress Graph Neural Architecture Search (GNAS). GNAS re-
quires intensive human effort and rich domain knowledge
to design search spaces and strategies. Leveraging powerful
knowledge and reasoning capabilities, LLMs can identify
suitable GNN frameworks within the search space of graph
neural network frameworks. GPT4GNAS [45] integrates GPT-
4 into GNAS, introducing a new set of prompts for GPT-4 to
guide it towards generating graph neural structures.
IX. B ENCHMARK DATASETS AND EVALUATIONS
In this section, we summarize benchmark datasets and
evaluation metrics for LLMs.
A. Datasets
This paper summarizes the popular and new datasets, the
LLM employed, the performed tasks, and the links to the open-
source code in the LLM-GGA area, as illustrated in Table V.
Below, we introduce commonly used benchmarks and the new
benchmarks proposed for the LLM-GGA field.
1) Popular datasets: Popular benchmark refers to a graph
benchmark that is widely and frequently used. We have sys-
tematically categorized these popular benchmarks according to
six directions, detailing which benchmarks are used for each
direction. Below are listed popular benchmarks commonly
used in the six directions.
•Graph structure understanding : ogbn-arxiv [56],
ogbn-products [56], Cora [100], CiteSeer [101],
Aminer(DBLP) [57], MetaQA [102], Wikidata5M [103],
PROTEINS [104], MUTAG [105], NCI1 [106], PTC
[107], Foursqure [108].
•Graph learning : ogbn-arxiv [56], ogbn-products [56],
ogb-papers110M [56], ogb-citation2 [56], Cora [100],
CiteSeer [101], Amazon-items [109], PubMed [110],
Reddit [111], CoraFull [112], Amazon [113], PROTEINS
[104], COX2 [114], BZR [114], OAG [115]
•Graph-formed reasoning : GSM8K [116], SV AMP
[117], FOLIO [118]
•Graph representation : Cora [100], CiteSeer [101],
Goodreads-books [119], PubMed [110], Amazon [113],
MIMIIC-III [120], Freebase [121], FB15K-237 [122]
•KG-based augmented retrieval : CWQ [123], WebQSP
[124], Wikidata [103]
•Graph-LLM-based applications : depending on specific
applications.
2) New datasets: More than existing datasets are needed to
explore LLMs’ ability to understand graph structures and their
potential to solve graph problems better. As a result, many
works have proposed new benchmarks to advance research in
this field, as shown in Table VI.
•GPR [59] contains 37 particular connected graph in-
stances generated by the Networkx toolkit, which include
the “bull graph,” “wheel graph,” “lollipop graph,” etc.These generated graph instances are relatively small, with
about 15 nodes and 28 links on average.
•GraphTMI [87] is a graph benchmark featuring a hi-
erarchy of graphs, associated prompts, and encoding
modalities. Different graph task difficulty depends on the
dual criteria of 1) count of motifs and 2) homophily in
the graph, which yields a dataset of EASY , MEDIUM,
and HARD graph problems.
•LLM4DyG [25] benchmark is to evaluate whether LLMs
are capable of understanding spatial-temporal information
on the dynamic graph. Nine dynamic graph tasks are
designed to assess LLMs’ abilities considering spatial and
temporal dimensions.
•GraphQA [86] comprises a set of diverse fundamental
graph problems with more varied and realistic graph
structures compared to previous studies in LLM research.
GraphQA is designed to measure the performance of
LLMs in graph data reasoning.
•NLGraph [27] benchmark is to examine whether lan-
guage models can reason with graphs and structures.
NLGraph contains eight graph structure understanding
tasks with varying algorithmic difficulties. Depending
on different network sizes, graph sparsity, and more,
NLGraph results in easy, medium, and hard subsets in
each graph reasoning task to enable difficulty scaling and
fine-grained analysis.
•GraphextQA [98] benchmark is a dataset for open do-
main question answering. It includes paired subgraphs
used to develop and evaluate graph language models.
The subgraphs are retrieved from Wikidata and contain
reasoning paths from entities mentioned in the questions
to the entities that the questions are asking about.
•CS-TAG [99] benchmark is a comprehensive and wide-
ranging compilation of benchmark datasets for TAGs.
This dataset encompasses a variety of challenging scenar-
ios, ranging from citation networks to purchase graphs.
The collection consists of eight distinct TAGs sourced
from diverse domains.
We also list which directions these new benchmarks are typ-
ically used for. For graph structure understanding, GPR [59],
GraphTMI [87], LLM4DyG [25], NLGraph [27], and CS-TAG
[99]can be used. For graph learning, CS-TAG [99] can be used.
For graph-formed reasoning, GraphextQA [98] can be used.
For graph representation, GraphTMI [87], GraphQA [86], and
CS-TAG [99] can be used. For KG-based augmented retrieval,
GraphextQA [98] can be used.
B. Evaluations
Evaluating the results of different tasks related to LLM-
GGA is also a critical issue. Thus, selecting evaluation metrics
to assess the results is essential to determining how well LLMs
perform their understanding of graphs and how effectively
models combining graphs and LLMs perform on various tasks
is vital. This section summarizes the metrics of different tasks,
shown as Table VII. Note that all test results related to LLMsTABLE V: A summary of LLM-GGA methods with datasets and source links.
Method Dataset LLM Task Link
InstrucGLM [78] ogbn-arxiv, Cora, PubMed Flan-T5 (instruction-finetune), Llama-
v1-7b (LoRA)Link, Node code link
GPT4Graph [23] ogbn-arxiv,Aminer,Wiki,MetaQA InstructGPT-3(frozen) Reasoning, Node, Graph code link
LLMtoGraph [24] generated by GPTs GPT-3.5-turbo, GPT-4, Wizard-Vicuna-
13B, 30B-Lazarus-Uncensored-HFMulti-hop Reasoning code link
Graph-LLM [73] ogbn-arxiv, Cora, PubMed, ogbn-products LLaMA, text-ada-embedding-002,
Palm-Cortex-001Node code link
TAPE [30] ogbn-arxiv, Cora, PubMed, ogbn-products GPT-3.5 Node code link
LLM4DyG [25] LLM4DyG GPT-3.5-turbo, Vicuna-7B, Vicuna-13B,
Llama-2-13B, CodeLlama-2-13BGraph -
GraphGPT [79] ogbn-arxiv, Cora, PubMed vicuna-7B-v1.1, vicuna-7B-v1.5 Node code link
GPPT [80] Cora, Reddit, CoraFull, Amazon-CoBuy,
ogbn-arxiv etc.- Link, Node code link
GraphPrompt [81] Flickr, PROTEINS, COX2, ENZYMES, BZR - Link, Node, Graph code link
All in one [82] Cora, CiteSeer, Reddit, Amazon, Pubmed - Link, Edge, Node, Graph code link
Graph-ToolFormer [59] GPR, Cora, Pubmed, Citeseer, PROTEINS,
MUTAG, NCI1, PTC, Twitter, FoursquareGPT-J-6B Q&A, Reasoning code link
RGV [83] GSM8K, SV AMP, ASDiv-a GPT-3.5-turbo math problems -
LLM-GNN [72] CORA, CITESEER, PUBMED, WIKICS,
OGBN-ARXIV , OGBN-PRODUCTSGPT-3.5-turbo Node code link
Which Modality should I use [87] Cora, Citeseer, Pubmed,GraphTMI GPT-4, GPT-4V Representation, Node -
WalkLM [38] PubMed, MIMIC-III, Freebase, FB15K-237 PLMs Representation, Node, Link code link
GraphText [37] Cora, Citeseer, Texas, Wisconsin, Cornell Llama-2-7B Node code link
TALK LIKE A GRAPH [86] GraphQA PaLM 2-XXS, PaLM 62B Node, Link -
Graph-guided CoT [84] 2WikiMultihopQA, MusiQue, Bamboogle Llama-2-13B,Llama-2-70B multi-hop question answering -
NLGraph [27] NLGraph TEXT-DA VINCI-003, GPT-3.5-
TURBO, CODE-DA VINCI-002, GPT-4Link,Node,Graph,Path,Pattern code link
Collaborative Query Rewriting [94] opportunity test sets, guardrail test set Dolly V2 Conversational Understanding -
WHEN AND WHY [26] OGBN-ARXIV , CORA, PUBMED, OGBN-
PRODUCT, ARXIV-2023ChatGPT Node code link
CR [35] FOLIO, LogiQA, ProofWriter, LogicalDe-
ductionGPT-3.5-turbo, GPT-4, LLaMA-13B,
LLaMA-65BLogic reasoning code link
SOCIALSENSE [95] RFPN, Twitter PLMs Response Forecasting code link
DGTL [39] Cora, PubMed, Books-History Llama-2-13B Node -
SHEGO [96] SGD, MultiWOZ 2.1 T5-small multi-domain DST -
Graph of Thought(GoT) [34] individual data GPT3.5(frozen) Graph-formed reasoning code link
GLEM [31] ogbnarxiv, ogbn-products, ogbn-papers100M PLMs Node code link
LPNL [77] OAG T5-base Link -
SIMTEG [71] OGBN-Arxiv, OGBN-Products, OGBL-
Citation2PLMs Node, link code link
Llmrec [44] Netflix, MovieLens gpt-3.5-turbo-16k Recommendation code link
ENG [75] OGB gpt-3.5-turbo Node generation -
OFA [32] OGBN-ARXIV , CORA PLMs Node, link, graph code link
G-prompt [33] OGBN-ARXIV , Instagram, Reddit PLMs Representation -
Beyond Text [74] OGBN-ARXIV , CORA, PubMed GPT-3.5, GPT-4 Node, link -
GPT4GNAS [45] OGBN-ARXIV , CORA, PubMed, Citeseer GPT-4 Graph neural architecture search -
Graphllm [64] NLGraph Llama2-7B, Llama2-13B Link, node, graph, path, pattern code link
G2P2 [85] Cora, Amazon PLMs Representation code link
ChatGraph [97] Gradio GPT-4V , Next-GPT Link, node, graph, application -
Graph Agent [76] Cora, PubMed GPT-4 Link, node, graph -
GoT* [36] AQUA-RAT, ScienceQA T5-base Graph-formed reasoning code link
KGP [93] HotpotQA, IIRC, 2WikiMQA, MuSiQue,
PDFTriage, RankLlama KG+LLM code link
Head-to-Tail [88] DBpredia, Movie, Book, Academics GPT-4 KG+LLM -
GLaM [40] DBLP, UMLS Llama-7B KG+LLM -
ToG [41] CWQ, WebQSP, GrailQA, QALD10-en, etc. GPT-3.5, GPT-4, Llama-2 KG+LLM code link
Autoalign [92] DBpedia, Wikidata ChatGPT, Claude KG+LLM code link
GNP [42] ConceptNet, UMLS, OpenBookQA, etc. FLAN-T5 xlarge (3B), xxlarge (11B) KG+LLM code link
RoG [43] WebQSP, CWQ, Freebase LLaMA2-7B KG+LLM code link
KGLens [91] Wikidata GPT-3.5-turbo, GPT-4, Babbage-002,
Davinci-002, Vicuna-33b-v1.3, Xwin-
LM-13B-V0.2, Yi-34B-ChatKG+LLM -TABLE VI: A summary of new datasets.
New Benchmark Link
GPR [59] https://github.com/jwzhanggy/Graph Toolformer/tree/main/data
GraphTMI [87] To be released
LLM4DyG [25] To be released
GraphQA [86] To be released
NLGraph [27] https://github.com/Arthur-Heng/NLGraph/tree/main/NLGraph
GraphextQA [98] https://huggingface.co/datasets/drt/graphext-qa
CS-TAG [99] https://github.com/sktsherlock/TAG-Benchmark
TABLE VII: Evaluations.
Tasks Metrics
Graph structure understanding task Accuracy, ROUGE, BLEU, Time cost, Comprehension, Correctness, Fidelity, Rectification
Comprehension
Graph learning task Accuracy, Macro-F1, Training Time, Tuned Parameters, GPU Occupy, Mismatch Rate,
Denial Rate, Token Limit Fraction
Graph resoning task Accuracy, F1-score, Precision, Recall, The Latency-V olume Trade-off, Number of errors
and cost
Graph representation depending on downstream tasks
KG-based augmented retrieval Accuracy, F1-score, Precision, Recall,
Graph-LLM-based applications depending on different tasks
in this paper are conducted using GPT-3.5 turbo or GPT-4
turbo.
1) Graph structure understanding task.: Several metrics are
usually used in graph structure understanding tasks: accuracy,
ROUGE [125], BLEU [126], Time cost, comprehension, cor-
rectness, fidelity, and rectification comprehension. Accuracy,
ROUGE, BLEU, and time cost are viral metrics. Meanwhile,
comprehension, correctness, fidelity, and rectification compre-
hension are new metrics [24] used to evaluate the ability
of LLMs to understand graphs through natural language,
the accuracy of solving graph problems, and the level of
confidence in the answers provided.
2) Graph learning task.: For graph learning tasks, when
evaluating a model, various metrics are considered to deter-
mine its effectiveness, efficiency, and computational demands.
When assessing the effectiveness of a model, metrics such
as accuracy, macro-F1, mismatch rate, and denial rate [87]
are considered. In terms of efficiency, metrics like training
time and tuned parameters are assessed. For computational
costs, metrics such as GPU occupancy and token limit fraction
are examined. Notably, the token limit fraction indicates the
proportion of tokens used compared to the maximum allowed
by the model’s constraints and can be formed as follows:
T =Number of usage tokens
Token limit constraint for the model(6)
3) Graph reasoning task.: When it comes to graph reason-
ing tasks, two main factors that are taken into consideration
are effectiveness and efficiency. Several metrics are used to
assess effectiveness, including accuracy, number of errors and
cost, F1-score, precision, and recall [127]. On the other hand,
efficiency is evaluated through metrics such as the Latency-
V olume Trade-off.
4) Graph representation.: The effectiveness of graph rep-
resentation is typically judged based on the performance of
downstream tasks that use this graph representation.5) Knowledge graph-based augmented retrieval.: Tasks in
the KG-based augmented retrieval direction typically involve
question-answering tasks. Evaluation metrics commonly used
include accuracy, precision, recall, F1-score, Hits@k [128],
EM [129], MSE, and for some generative tasks, human eval-
uation may also be utilized.
X. F UTURE DIRECTIONS
The above survey of the state-of-the-art LLM-GGA research
reveals a promising and young research field. The following
section discusses exciting directions for future work.
A. More Complex Graph Problems
More complex graph tasks. Can LLMs solve graph al-
gorithm problems? Existing works on traditional graph tasks
are based on fundamental graph problems such as shortest
path, clustering coefficient computing, maximum flow, etc.
However, can LLMs address NP problems such as community
search, interactive graph problems, or even NP-hard problems,
and if so, how can they tackle them? For graph learning tasks,
current research primarily focuses on simple node, edge, and
graph classification. Future work can focus on more complex
graph learning problems, such as the diverse classification
outcomes arising from isomorphic and heterogeneous graphs.
More complex graph patterns. Graphs contain various
graph patterns, each with its explicit definition and unique
characteristics, such as stars, triangles, cliques, butterflies, and
more. Therefore, recognizing graph patterns and utilizing their
characteristics to solve downstream tasks can be highly advan-
tageous. Currently, only limited works leverage the properties
of stars, triangles, and cliques to solve problems.
Furthermore, understanding graph data still remains a sig-
nificant challenge for existing LLMs, limiting their ability to
tackle more complex graph problems. Therefore, incorporating
LLMs into the process is a promising direction for solving
more complex graph problems.B. LLM Exploration on Diverse Graphs
Most existing work mainly focuses on static graphs, while
there exists a wide range of different graphs, including
undirected, directed, cyclic, acyclic, isomorphic, heteroge-
neous, dynamic, etc. Different types of graphs have significant
structural differences, such as static graphs, dynamic graphs,
temporal graphs, uncertain graphs, heterogeneous graphs, etc.
Specifically, unlike static graphs, dynamic graphs can be
represented as ordered lists or asynchronous streams of timed
events, capturing patterns of temporal network evolution, such
as the addition or removal of nodes and edges. Evaluating the
ability of LLMs to understand the spatio-temporal information
of dynamic graphs is crucial for web applications. Evaluating
whether LLMs can determine when nodes are connected,
identify which nodes are connected to a given node at a
specific time, and find a chronological path by combining
temporal and spatial information is essential to assessing
LLMs’ understanding of dynamic graphs. Future work can
further explore other types of graphs, such as dynamic graphs
and temporal graphs, address problems like maximum flow,
and predict the evolution of graphs.
Moreover, existing studies have conflicting views on the
LLM graph reasoning ability, with some presenting contradic-
tory findings. This ambiguity could be due to various factors,
including dataset selection, diverse prompt engineering tech-
niques, the range of graph reasoning tasks, and the utilization
of different LLM models.
C. Better LLM-GNN Pipelines
GNNs are designed to handle structural information by
continuously learning information from surrounding subgraphs
through aggregation functions. On the other hand, LLMs excel
in processing textual information, text reasoning, semantic
understanding, and more. The challenge lies in leveraging both
advantages to enable a pipeline that can effectively handle both
attributed and pure graphs. If GNNs and LLMs are simply
stacked, the parameter size of GNNs is notably smaller than
that of LLMs. This discrepancy may result in the issue of
vanishing gradients during training, as mentioned in [130],
which can impede the iterative updating process of GNNs.
Additionally, GNNs need to utilize the extensive knowledge
contained within LLMs fully, and they cannot effectively
extract specific knowledge tailored for specific downstream
tasks in different graphs.
D. Graph Foundation Model
LLM is undoubtedly the foundational model in NLP. Can
we draw inspiration from LLMs to train a graph foundation
model? For example, can training strategies like instruction
tuning and DPO be applied to tasks involving graphs? The
current research has primarily introduced graph foundation
models in the form of LLM-GNN pipelines and graph-aware
tuning LLMs. Future endeavors can focus on exploring graph
foundation models better suited for tasks involving graphs.E. Better Graph Prompts
Most graph prompts are currently designed based on GNNs,
with only a few works focusing on LLMs. Graph prompts for
LLMs have yet to be sufficiently explored.
Graph Prompt for GNNs. The typical approach uses
simple concatenation, addition, or dot product operations with
trainable parameters. Some existing works have considered
more complex fusion methods, such as [82], which assumes
the structural features of graph prompts. However, compared
to the combination of prompts and pretexts, the variety of
graph prompts and pre-graphs is still in the exploratory stage.
Graph-enhanced Prompts for LLMs. Relying solely on
manual prompts and self-prompting has limited capabilities
in improving model performance, as they only explore the
existing abilities of LLM. As shown in Section III-C, LLMs
can be trained as agents to utilize tools for graph tasks that are
hard to solve, like API call prompt [59]. GoT [130] is also
a graph reasoning paradigm that enables LLMs to provide
correct answers. Future work based on the graph reasoning
paradigm can consider cost-effective approaches for GoT,
such as pruning and tricks to reduce algorithm complexity.
In the future, it would be beneficial to explore simpler GoT
paradigms that can improve the effectiveness of LLMs.
F . Modal Alignment
Modal alignment refers to the alignment between two
modalities: text and graph. The input for LLMs is typically
sequential data, often text. Graph and text are two different
modalities, and studying the alignment between these two
modalities for LLMs involves finding a shared mapping feature
space for graphs and text. The shared mapping space allows
LLMs to understand graph data similarly to how they know
textual information if they comprehend text.
G. Explainabilily
GNNs are currently widely used for solving complex graph
problems. However, they need more interpretability, which
hinders their practical application. On the other hand, LLMs
possess reasoning capabilities and have succeeded in various
natural language processing tasks. The combination of LLMs
and GNNs has the potential to offer a more transparent ap-
proach to solving graph problems by leveraging the reasoning
abilities of LLMs. If the combination of LLMs and GNNs
is interpretable, it can be utilized for various tasks., including
recommendation systems, drug discovery, and fraud detection.
This combination can lead to the development of more reliable
and efficient decision-making systems across various domains.
H. Efficiency on Large-scale Graphs
Due to the limited input length of LLM, the graph sizes
inputted through prompts typically consist of dozens of nodes.
However, for large graphs with tens of thousands of nodes and
edges, how can LLMs with limited input length solve such
large graphs? A larger input window is required in the case of
attributed graphs, where both node and edge attributes need to
be considered along with the graph structure. How does LLMaddress this case? There are currently few effective methods
to enable LLM to handle them.
XI. C ONCLUSIONS
LLM-GGA has emerged as a promising field that has
garnered significant attention from researchers. This paper
introduces a comprehensive structural taxonomy based on
recent research, which classifies LLM-GGA research into three
main directions: LLM-GQP, LLM-GIL, and graph-LLM-based
applications. LLM-GQP encompasses graph understanding
and KG-based augmented retrieval, while LLM-GIL involves
graph learning, graph-formed reasoning, and graph represen-
tation. The motivation, challenges, and mainstream methods
of each direction are thoroughly examined.
For the six mentioned directions, a comparison of various
methods was conducted to explore their potential in each
area. It is observed that LLM shows preliminary capabilities
in structural understanding, addressing issues like maximum
flow and bipartite graph matching over small graphs. However,
it is susceptible to factors such as node degree and graph
density, leading to potential misjudgments in graph connec-
tivity. Additionally, LLM proves beneficial for graph learning
tasks due to its strong semantic understanding and reasoning
abilities, coupled with learning from extensive corpora, which
can provide external knowledge to GNNs and aid in semantic
information comprehension, learning, and reasoning. Thanks
to LLM’s semantic understanding capabilities, graph represen-
tation can achieve deeper semantic embeddings. The discus-
sion also delves into KG-based augmented retrieval to enhance
LLMs retrieval and factual knowledge-answering abilities. The
paper summarizes over 40 datasets, evaluation metrics for six
directions, and source code for over 30 mainstream methods in
these directions. It highlights the existing challenges in current
methods and proposes future directions to guide and motivate
further research in the LLM-GGA field.
REFERENCES
[1] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester,
N. Du, A. M. Dai, and Q. V . Le, “Finetuned Language Models
Are Zero-Shot Learners,” Feb. 2022, arXiv:2109.01652 [cs]. [Online].
Available: http://arxiv.org/abs/2109.01652
[2] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction Tuning
with GPT-4,” Apr. 2023, arXiv:2304.03277 [cs]. [Online]. Available:
http://arxiv.org/abs/2304.03277
[3] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,
and C. Finn, “Direct preference optimization: Your language
model is secretly a reward model,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html
[4] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, Y . Li, C. Gao, Y . Huang,
W. Lyu, Y . Zhang, X. Li, Z. Liu, Y . Liu, Y . Wang, Z. Zhang, B. Vidgen,
B. Kailkhura, C. Xiong, C. Xiao, C. Li, E. Xing, F. Huang, H. Liu,
H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis, M. Zitnik, M. Jiang,
M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao, J. Tang,
J. Wang, J. Vanschoren, J. Mitchell, K. Shu, K. Xu, K.-W. Chang,
L. He, L. Huang, M. Backes, N. Z. Gong, P. S. Yu, P.-Y . Chen, Q. Gu,
R. Xu, R. Ying, S. Ji, S. Jana, T. Chen, T. Liu, T. Zhou, W. Wang,
X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang, Y . Liu,Y . Ye, Y . Cao, Y . Chen, and Y . Zhao, “TrustLLM: Trustworthiness in
Large Language Models,” Mar. 2024, arXiv:2401.05561 [cs]. [Online].
Available: http://arxiv.org/abs/2401.05561
[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,
C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,
J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn,
S. Hosseini, R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa,
I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,
D. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P. Mishra,
I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi,
A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,
B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov,
Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,
S. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned
chat models,” CoRR , vol. abs/2307.09288, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2307.09288
[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,
P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,
J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,
P. F. Christiano, J. Leike, and R. Lowe, “Training language
models to follow instructions with human feedback,” in Advances in
Neural Information Processing Systems 35: Annual Conference on
Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022 , S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds.,
2022. [Online]. Available: http://papers.nips.cc/paper files/paper/2022/
hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html
[7] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa:
A dataset for LLM question answering with external tools,” in
Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 - 16, 2023 , A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
Eds., 2023. [Online]. Available: http://papers.nips.cc/paper files/paper/
2023/hash/9cb2a7495900f8b602cb10159246a016-Abstract-Datasets
and Benchmarks.html
[8] Z. Li, S. Fan, Y . Gu, X. Li, Z. Duan, B. Dong, N. Liu, and J. Wang,
“Flexkbqa: A flexible llm-powered framework for few-shot knowledge
base question answering,” in Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 38, no. 17, 2024, pp. 18 608–18 616.
[9] B. Zhang, B. Haddow, and A. Birch, “Prompting large language model
for machine translation: A case study,” in International Conference on
Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,
USA, ser. Proceedings of Machine Learning Research, A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
Eds., vol. 202. PMLR, 2023, pp. 41 092–41 110. [Online]. Available:
https://proceedings.mlr.press/v202/zhang23m.html
[10] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is your code
generated by chatgpt really correct? rigorous evaluation of large
language models for code generation,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html
[11] A. Ni, S. Iyer, D. Radev, V . Stoyanov, W. Yih, S. I. Wang,
and X. V . Lin, “LEVER: learning to verify language-to-code
generation with execution,” in International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA ,
ser. Proceedings of Machine Learning Research, A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
Eds., vol. 202. PMLR, 2023, pp. 26 106–26 128. [Online]. Available:
https://proceedings.mlr.press/v202/ni23b.html
[12] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and
T. Eliassi-Rad, “Collective Classification in Network Data,” AI
Magazine , vol. 29, no. 3, pp. 93–106, Sep. 2008. [Online]. Available:
https://onlinelibrary.wiley.com/doi/10.1609/aimag.v29i3.2157
[13] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
1024–1034. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html
[14] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S.
Pappu, K. Leswing, and V . Pande, “Moleculenet: a benchmark for
molecular machine learning,” Chemical science , vol. 9, no. 2, pp. 513–
530, 2018.
[15] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan,
R. Stata, A. Tomkins, and J. Wiener, “Graph structure in the
Web,” Computer Networks , vol. 33, no. 1-6, pp. 309–320, Jun.
2000. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
S1389128600000839
[16] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” in 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings . OpenReview.net, 2017. [Online].
Available: https://openreview.net/forum?id=SJU4ayYgl
[17] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li `o,
and Y . Bengio, “Graph attention networks,” in 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings .
OpenReview.net, 2018. [Online]. Available: https://openreview.net/
forum?id=rJXMpikCZ
[18] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
“Neural message passing for quantum chemistry,” in Proceedings
of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017 , ser. Proceedings
of Machine Learning Research, D. Precup and Y . W. Teh,
Eds., vol. 70. PMLR, 2017, pp. 1263–1272. [Online]. Available:
http://proceedings.mlr.press/v70/gilmer17a.html
[19] Y . Hong, J. W. Lam, and B. Z. Tang, “Aggregation-induced emission:
phenomenon, mechanism and applications,” Chemical communications ,
no. 29, pp. 4332–4353, 2009.
[20] W. Cong, M. Ramezani, and M. Mahdavi, “On provable benefits
of depth in training graph convolutional networks,” in Advances
in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual , M. Ranzato, A. Beygelzimer, Y . N.
Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 9936–
9949. [Online]. Available: https://proceedings.neurips.cc/paper/2021/
hash/524265e8b942930fbbe8a5d979d29205-Abstract.html
[21] S. Fan, X. Wang, C. Shi, P. Cui, and B. Wang, “Generalizing Graph
Neural Networks on Out-of-Distribution Graphs,” IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 46, no. 1, pp.
322–337, Jan. 2024. [Online]. Available: https://ieeexplore.ieee.org/
document/10268633/
[22] J. Liu, Z. Shen, Y . He, X. Zhang, R. Xu, H. Yu, and P. Cui,
“Towards Out-Of-Distribution Generalization: A Survey,” Jul. 2023,
arXiv:2108.13624 [cs]. [Online]. Available: http://arxiv.org/abs/2108.
13624
[23] J. Guo, L. Du, H. Liu, M. Zhou, X. He, and S. Han, “GPT4Graph:
Can Large Language Models Understand Graph Structured Data ? An
Empirical Evaluation and Benchmarking,” Jul. 2023, arXiv:2305.15066
[cs]. [Online]. Available: http://arxiv.org/abs/2305.15066
[24] C. Liu and B. Wu, “Evaluating Large Language Models on
Graphs: Performance Insights and Comparative Analysis,” Sep. 2023,
arXiv:2308.11224 [cs]. [Online]. Available: http://arxiv.org/abs/2308.
11224
[25] Z. Zhang, X. Wang, Z. Zhang, H. Li, Y . Qin, and W. Zhu,
“LLM4DyG: Can Large Language Models Solve Spatial-Temporal
Problems on Dynamic Graphs?” Mar. 2024, arXiv:2310.17110 [cs].
[Online]. Available: http://arxiv.org/abs/2310.17110
[26] J. Huang, X. Zhang, Q. Mei, and J. Ma, “Can llms effectively
leverage graph structural information: When and why,” CoRR , vol.
abs/2309.16595, 2023. [Online]. Available: https://doi.org/10.48550/
arXiv.2309.16595
[27] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y . Tsvetkov, “Can
Language Models Solve Graph Problems in Natural Language?” Jan.
2024, arXiv:2305.10037 [cs]. [Online]. Available: http://arxiv.org/abs/
2305.10037
[28] Q. Dong, L. Dong, K. Xu, G. Zhou, Y . Hao, Z. Sui, and
F. Wei, “Large Language Model for Science: A Study on P
vs. NP,” Sep. 2023, arXiv:2309.05689 [cs]. [Online]. Available:
http://arxiv.org/abs/2309.05689[29] L. Fan, W. Hua, L. Li, H. Ling, and Y . Zhang, “NPHardEval:
Dynamic Benchmark on Reasoning Ability of Large Language Models
via Complexity Classes,” Feb. 2024, arXiv:2312.14890 [cs]. [Online].
Available: http://arxiv.org/abs/2312.14890
[30] X. He, X. Bresson, T. Laurent, A. Perold, Y . LeCun, and
B. Hooi, “Harnessing Explanations: LLM-to-LM Interpreter for
Enhanced Text-Attributed Graph Representation Learning,” Mar. 2024,
arXiv:2305.19523 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
19523
[31] J. Zhao, M. Qu, C. Li, H. Yan, Q. Liu, R. Li, X. Xie, and J. Tang,
“Learning on Large-scale Text-attributed Graphs via Variational
Inference,” Mar. 2023, arXiv:2210.14709 [cs]. [Online]. Available:
http://arxiv.org/abs/2210.14709
[32] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y . Chen, and
M. Zhang, “One for All: Towards Training One Graph Model for
All Classification Tasks,” Dec. 2023, arXiv:2310.00149 [cs]. [Online].
Available: http://arxiv.org/abs/2310.00149
[33] X. Huang, K. Han, D. Bao, Q. Tao, Z. Zhang, Y . Yang, and Q. Zhu,
“Prompt-based Node Feature Extractor for Few-shot Learning on
Text-Attributed Graphs,” Sep. 2023, arXiv:2309.02848 [cs]. [Online].
Available: http://arxiv.org/abs/2309.02848
[34] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk,
and T. Hoefler, “Graph of Thoughts: Solving Elaborate Problems with
Large Language Models,” Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 38, no. 16, pp. 17 682–17 690, Mar. 2024,
arXiv:2308.09687 [cs]. [Online]. Available: http://arxiv.org/abs/2308.
09687
[35] Y . Zhang, J. Yang, Y . Yuan, and A. C.-C. Yao, “Cumulative Reasoning
with Large Language Models,” Apr. 2024, arXiv:2308.04371 [cs].
[Online]. Available: http://arxiv.org/abs/2308.04371
[36] Y . Yao, Z. Li, and H. Zhao, “Beyond Chain-of-Thought, Effective
Graph-of-Thought Reasoning in Language Models,” Mar. 2024,
arXiv:2305.16582 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
16582
[37] J. Zhao, L. Zhuo, Y . Shen, M. Qu, K. Liu, M. Bronstein,
Z. Zhu, and J. Tang, “GraphText: Graph Reasoning in Text
Space,” Oct. 2023, arXiv:2310.01089 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.01089
[38] Y . Tan, Z. Zhou, H. Lv, W. Liu, and C. Yang,
“Walklm: A uniform language model fine-tuning framework for
attributed graph embedding,” in Advances in Neural Information
Processing Systems 36: Annual Conference on Neural Information
Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/2ac879d1865475a7abc8dfc7a9c15c27-Abstract-Conference.html
[39] Y . Qin, X. Wang, Z. Zhang, and W. Zhu, “Disentangled Representation
Learning with Large Language Models for Text-Attributed Graphs,”
Mar. 2024, arXiv:2310.18152 [cs]. [Online]. Available: http://arxiv.
org/abs/2310.18152
[40] S. Dernbach, K. Agarwal, A. Zuniga, M. Henry, and S. Choudhury,
“GLaM: Fine-Tuning Large Language Models for Domain Knowledge
Graph Alignment via Neighborhood Partitioning and Generative
Subgraph Encoding,” Apr. 2024, arXiv:2402.06764 [cs]. [Online].
Available: http://arxiv.org/abs/2402.06764
[41] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y . Gong, L. M. Ni,
H.-Y . Shum, and J. Guo, “Think-on-Graph: Deep and Responsible
Reasoning of Large Language Model on Knowledge Graph,” Mar.
2024, arXiv:2307.07697 [cs]. [Online]. Available: http://arxiv.org/abs/
2307.07697
[42] Y . Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V .
Chawla, and P. Xu, “Graph Neural Prompting with Large Language
Models,” Dec. 2023, arXiv:2309.15427 [cs]. [Online]. Available:
http://arxiv.org/abs/2309.15427
[43] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on Graphs:
Faithful and Interpretable Large Language Model Reasoning,” Feb.
2024, arXiv:2310.01061 [cs]. [Online]. Available: http://arxiv.org/abs/
2310.01061
[44] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang,
D. Yin, and C. Huang, “LLMRec: Large Language Models with Graph
Augmentation for Recommendation,” Jan. 2024, arXiv:2311.00423
[cs]. [Online]. Available: http://arxiv.org/abs/2311.00423[45] H. Wang, Y . Gao, X. Zheng, P. Zhang, H. Chen, J. Bu,
and P. S. Yu, “Graph Neural Architecture Search with GPT-
4,” Mar. 2024, arXiv:2310.01436 [cs]. [Online]. Available: http:
//arxiv.org/abs/2310.01436
[46] L. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen, “Exploring
large language model for graph data understanding in online job
recommendations,” in Thirty-Eighth AAAI Conference on Artificial
Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative
Applications of Artificial Intelligence, IAAI 2024, Fourteenth
Symposium on Educational Advances in Artificial Intelligence, EAAI
2014, February 20-27, 2024, Vancouver, Canada , M. J. Wooldridge,
J. G. Dy, and S. Natarajan, Eds. AAAI Press, 2024, pp. 9178–9186.
[Online]. Available: https://doi.org/10.1609/aaai.v38i8.28769
[47] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,
B. Zhang, J. Zhang, Z. Dong, Y . Du, C. Yang, Y . Chen, Z. Chen,
J. Jiang, R. Ren, Y . Li, X. Tang, Z. Liu, P. Liu, J.-Y . Nie, and J.-R. Wen,
“A Survey of Large Language Models,” Nov. 2023, arXiv:2303.18223
[cs]. [Online]. Available: http://arxiv.org/abs/2303.18223
[48] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and
X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt
and beyond,” CoRR , vol. abs/2304.13712, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2304.13712
[49] M. Himsolt, “Gml: A portable graph file format,” Technical report,
Universitat Passau, Tech. Rep., 1997.
[50] U. Brandes, M. Eiglsperger, J. Lerner, and C. Pich, “Graph markup lan-
guage (graphml),” in Handbook on Graph Drawing and Visualization ,
R. Tamassia, Ed. Chapman and Hall/CRC, 2013, pp. 517–541.
[51] N. Francis, A. Green, P. Guagliardo, L. Libkin, T. Lindaaker,
V . Marsault, S. Plantikow, M. Rydberg, P. Selmer, and A. Taylor,
“Cypher: An Evolving Query Language for Property Graphs,” in
Proceedings of the 2018 International Conference on Management of
Data . Houston TX USA: ACM, May 2018, pp. 1433–1445. [Online].
Available: https://dl.acm.org/doi/10.1145/3183713.3190657
[52] M. A. Rodriguez, “The Gremlin graph traversal machine and language
(invited talk),” in Proceedings of the 15th Symposium on Database
Programming Languages . Pittsburgh PA USA: ACM, Oct. 2015,
pp. 1–10. [Online]. Available: https://dl.acm.org/doi/10.1145/2815072.
2815073
[53] J. P ´erez, M. Arenas, and C. Gutierrez, “Semantics and complexity of
SPARQL,” ACM Transactions on Database Systems , vol. 34, no. 3,
pp. 1–45, Aug. 2009. [Online]. Available: https://dl.acm.org/doi/10.
1145/1567274.1567278
[54] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, “RAT-
SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL
Parsers,” Aug. 2021, arXiv:1911.04942 [cs]. [Online]. Available:
http://arxiv.org/abs/1911.04942
[55] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,
“Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
Methods in Natural Language Processing,” ACM Computing Surveys ,
vol. 55, no. 9, pp. 1–35, Sep. 2023. [Online]. Available: https:
//dl.acm.org/doi/10.1145/3560815
[56] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta,
and J. Leskovec, “Open graph benchmark: Datasets for machine
learning on graphs,” in Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual ,
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,
2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/
hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html
[57] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su,
“ArnetMiner: extraction and mining of academic social networks,”
inProceedings of the 14th ACM SIGKDD international conference
on Knowledge discovery and data mining . Las Vegas Nevada
USA: ACM, Aug. 2008, pp. 990–998. [Online]. Available: https:
//dl.acm.org/doi/10.1145/1401890.1402008
[58] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli,
E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom,
“Toolformer: Language models can teach themselves to use tools,”
inAdvances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 - 16, 2023 , A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html[59] J. Zhang, “Graph-ToolFormer: To Empower LLMs with Graph
Reasoning Ability via Prompt Augmented by ChatGPT,” May 2023,
arXiv:2304.11116 [cs]. [Online]. Available: http://arxiv.org/abs/2304.
11116
[60] H. Face, “hivemind/gpt-j-6b-8bit.”
[61] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter
autoregressive language model,” 2021.
[62] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “LLaMA: Open and Efficient
Foundation Language Models,” 2023, publisher: [object Object]
Version Number: 1. [Online]. Available: https://arxiv.org/abs/2302.
13971
[63] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,
and W. Chen, “LoRA: Low-Rank Adaptation of Large Language
Models,” Oct. 2021, arXiv:2106.09685 [cs]. [Online]. Available:
http://arxiv.org/abs/2106.09685
[64] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y . Yang,
“GraphLLM: Boosting Graph Reasoning Ability of Large Language
Model,” Oct. 2023, arXiv:2310.05845 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.05845
[65] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,
E. H. Chi, Q. V . Le, and D. Zhou, “Chain-of-thought prompting
elicits reasoning in large language models,” in Advances in Neural
Information Processing Systems 35: Annual Conference on Neural
Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022 , S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022.
[Online]. Available: http://papers.nips.cc/paper files/paper/2022/hash/
9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
[66] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-
hery, and D. Zhou, “Self-Consistency Improves Chain of Thought
Reasoning in Language Models,” Mar. 2023, arXiv:2203.11171 [cs].
[Online]. Available: http://arxiv.org/abs/2203.11171
[67] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu,
L. Li, and Z. Sui, “A Survey on In-context Learning,” Jun. 2023,
arXiv:2301.00234 [cs]. [Online]. Available: http://arxiv.org/abs/2301.
00234
[68] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal Policy Optimization Algorithms,” Aug. 2017, arXiv:1707.06347
[cs]. [Online]. Available: http://arxiv.org/abs/1707.06347
[69] H. Dong, W. Xiong, D. Goyal, Y . Zhang, W. Chow, R. Pan,
S. Diao, J. Zhang, K. Shum, and T. Zhang, “RAFT: Reward
rAnked FineTuning for Generative Foundation Model Alignment,”
Dec. 2023, arXiv:2304.06767 [cs, stat]. [Online]. Available: http:
//arxiv.org/abs/2304.06767
[70] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y . Li, and H. Wang, “Preference
Ranking Optimization for Human Alignment,” Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 38, no. 17, pp.
18 990–18 998, Mar. 2024. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/29865
[71] K. Duan, Q. Liu, T.-S. Chua, S. Yan, W. T. Ooi, Q. Xie, and J. He,
“SimTeG: A Frustratingly Simple Approach Improves Textual Graph
Learning,” Aug. 2023, arXiv:2308.02565 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.02565
[72] Z. Chen, H. Mao, H. Wen, H. Han, W. Jin, H. Zhang, H. Liu, and
J. Tang, “Label-free node classification on graphs with large language
models (LLMS),” CoRR , vol. abs/2310.04668, 2023. [Online].
Available: https://doi.org/10.48550/arXiv.2310.04668
[73] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang,
D. Yin, W. Fan, H. Liu, and J. Tang, “Exploring the Potential of
Large Language Models (LLMs) in Learning on Graphs,” Jan. 2024,
arXiv:2307.03393 [cs]. [Online]. Available: http://arxiv.org/abs/2307.
03393
[74] Y . Hu, Z. Zhang, and L. Zhao, “Beyond Text: A Deep Dive
into Large Language Models’ Ability on Understanding Graph
Data,” Oct. 2023, arXiv:2310.04944 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.04944
[75] J. Yu, Y . Ren, C. Gong, J. Tan, X. Li, and X. Zhang, “Empower
Text-Attributed Graphs Learning with Large Language Models
(LLMs),” Oct. 2023, arXiv:2310.09872 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.09872
[76] Q. Wang, Z. Gao, and R. Xu, “Graph agent: Explicit reasoning agentfor graphs,” CoRR , vol. abs/2310.16421, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2310.16421
[77] B. Bi, S. Liu, Y . Wang, L. Mei, and X. Cheng, “LPNL: Scalable Link
Prediction with Large Language Models,” Feb. 2024, arXiv:2401.13227
[cs]. [Online]. Available: http://arxiv.org/abs/2401.13227
[78] R. Ye, C. Zhang, R. Wang, S. Xu, and Y . Zhang, “Language is All a
Graph Needs,” Feb. 2024, arXiv:2308.07134 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.07134
[79] J. Tang, Y . Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and
C. Huang, “GraphGPT: Graph Instruction Tuning for Large Language
Models,” Dec. 2023, arXiv:2310.13023 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.13023
[80] M. Sun, K. Zhou, X. He, Y . Wang, and X. Wang, “GPPT:
Graph Pre-training and Prompt Tuning to Generalize Graph Neural
Networks,” in Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining . Washington DC
USA: ACM, Aug. 2022, pp. 1717–1727. [Online]. Available:
https://dl.acm.org/doi/10.1145/3534678.3539249
[81] Z. Liu, X. Yu, Y . Fang, and X. Zhang, “GraphPrompt: Unifying
Pre-Training and Downstream Tasks for Graph Neural Networks,”
inProceedings of the ACM Web Conference 2023 . Austin
TX USA: ACM, Apr. 2023, pp. 417–428. [Online]. Available:
https://dl.acm.org/doi/10.1145/3543507.3583386
[82] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, “All in One: Multi-Task
Prompting for Graph Neural Networks,” in Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining .
Long Beach CA USA: ACM, Aug. 2023, pp. 2120–2131. [Online].
Available: https://dl.acm.org/doi/10.1145/3580305.3599256
[83] L. Cao, “GraphReason: Enhancing Reasoning Capabilities of Large
Language Models through A Graph-Based Verification Approach,”
Apr. 2024, arXiv:2308.09267 [cs]. [Online]. Available: http://arxiv.
org/abs/2308.09267
[84] J. Park, A. Patel, O. Z. Khan, H. J. Kim, and J.-K. Kim, “Graph-
Guided Reasoning for Multi-Hop Question Answering in Large
Language Models,” Nov. 2023, arXiv:2311.09762 [cs]. [Online].
Available: http://arxiv.org/abs/2311.09762
[85] Z. Wen and Y . Fang, “Augmenting Low-Resource Text Classification
with Graph-Grounded Pre-training and Prompting,” in Proceedings
of the 46th International ACM SIGIR Conference on Research
and Development in Information Retrieval , Jul. 2023, pp. 506–516,
arXiv:2305.03324 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
03324
[86] B. Fatemi, J. Halcrow, and B. Perozzi, “Talk like a Graph: Encoding
Graphs for Large Language Models,” Oct. 2023, arXiv:2310.04560
[cs]. [Online]. Available: http://arxiv.org/abs/2310.04560
[87] D. Das, I. Gupta, J. Srivastava, and D. Kang, “Which Modality
should I use – Text, Motif, or Image? : Understanding Graphs with
Large Language Models,” Mar. 2024, arXiv:2311.09862 [cs]. [Online].
Available: http://arxiv.org/abs/2311.09862
[88] K. Sun, Y . E. Xu, H. Zha, Y . Liu, and X. L. Dong, “Head-to-Tail: How
Knowledgeable are Large Language Models (LLMs)? A.K.A. Will
LLMs Replace Knowledge Graphs?” Apr. 2024, arXiv:2308.10168
[cs]. [Online]. Available: http://arxiv.org/abs/2308.10168
[89] L. Yang, H. Chen, Z. Li, X. Ding, and X. Wu, “Give us the facts:
Enhancing large language models with knowledge graphs for fact-
aware language modeling,” IEEE Transactions on Knowledge and Data
Engineering , 2024.
[90] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, “Unifying
large language models and knowledge graphs: A roadmap,” CoRR ,
vol. abs/2306.08302, 2023. [Online]. Available: https://doi.org/10.
48550/arXiv.2306.08302
[91] S. Zheng, H. Bai, Y . Zhang, Y . Su, X. Niu, and N. Jaitly, “KGLens:
A Parameterized Knowledge Graph Solution to Assess What an LLM
Does and Doesn’t Know,” Feb. 2024, arXiv:2312.11539 [cs]. [Online].
Available: http://arxiv.org/abs/2312.11539
[92] R. Zhang, Y . Su, B. D. Trisedya, X. Zhao, M. Yang, H. Cheng,
and J. Qi, “AutoAlign: Fully Automatic and Effective Knowledge
Graph Alignment enabled by Large Language Models,” Nov. 2023,
arXiv:2307.11772 [cs]. [Online]. Available: http://arxiv.org/abs/2307.
11772
[93] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and
T. Derr, “Knowledge Graph Prompting for Multi-Document Question
Answering,” Dec. 2023, arXiv:2308.11730 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.11730[94] Z. Chen, Z. Jiang, F. Yang, E. Cho, X. Fan, X. Huang, Y . Lu,
and A. Galstyan, “Graph Meets LLM: A Novel Approach to
Collaborative Filtering for Robust Conversational Understanding,”
Jun. 2023, arXiv:2305.14449 [cs]. [Online]. Available: http://arxiv.org/
abs/2305.14449
[95] C. Sun, J. Li, Y . R. Fung, H. P. Chan, T. Abdelzaher, C. Zhai, and
H. Ji, “Decoding the Silent Majority: Inducing Belief Augmented
Social Graph with Large Language Model for Response Forecasting,”
Oct. 2023, arXiv:2310.13297 [cs]. [Online]. Available: http://arxiv.org/
abs/2310.13297
[96] R. Su, T.-W. Wu, and B.-H. Juang, “Schema Graph-Guided Prompt for
Multi-Domain Dialogue State Tracking,” Nov. 2023, arXiv:2311.06345
[cs]. [Online]. Available: http://arxiv.org/abs/2311.06345
[97] Y . Peng, S. Lin, Q. Chen, L. Xu, X. Ren, Y . Li, and J. Xu,
“ChatGraph: Chat with Your Graphs,” Jan. 2024, arXiv:2401.12672
[cs]. [Online]. Available: http://arxiv.org/abs/2401.12672
[98] Y . Shen, R. Liao, Z. Han, Y . Ma, and V . Tresp, “GraphextQA:
A Benchmark for Evaluating Graph-Enhanced Large Language
Models,” Oct. 2023, arXiv:2310.08487 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.08487
[99] H. Yan, C. Li, R. Long, C. Yan, J. Zhao, W. Zhuang, J. Yin,
P. Zhang, W. Han, H. Sun, W. Deng, Q. Zhang, L. Sun,
X. Xie, and S. Wang, “A comprehensive study on text-attributed
graphs: Benchmarking and rethinking,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets and
Benchmarks.html
[100] A. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating
the construction of internet portals with machine learning,” Inf.
Retr., vol. 3, no. 2, pp. 127–163, 2000. [Online]. Available:
https://doi.org/10.1023/A:1009953814988
[101] C. L. Giles, K. D. Bollacker, and S. Lawrence, “CiteSeer: an
automatic citation indexing system,” in Proceedings of the third ACM
conference on Digital libraries - DL ’98 . Pittsburgh, Pennsylvania,
United States: ACM Press, 1998, pp. 89–98. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=276675.276685
[102] Y . Zhang, H. Dai, Z. Kozareva, A. Smola, and L. Song,
“Variational Reasoning for Question Answering With Knowledge
Graph,” Proceedings of the AAAI Conference on Artificial Intelligence ,
vol. 32, no. 1, Apr. 2018. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/12057
[103] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and
J. Tang, “KEPLER: A unified model for knowledge embedding
and pre-trained language representation,” Trans. Assoc. Comput.
Linguistics , vol. 9, pp. 176–194, 2021. [Online]. Available: https:
//doi.org/10.1162/tacl a00360
[104] K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V . N. Vishwanathan,
A. J. Smola, and H.-P. Kriegel, “Protein function prediction via
graph kernels,” Bioinformatics , vol. 21, no. Suppl 1, pp. i47–i56, Jun.
2005. [Online]. Available: https://academic.oup.com/bioinformatics/
article-lookup/doi/10.1093/bioinformatics/bti1007
[105] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shuster-
man, and C. Hansch, “Structure-activity relationship of mutagenic aro-
matic and heteroaromatic nitro compounds. correlation with molecular
orbital energies and hydrophobicity,” Journal of medicinal chemistry ,
vol. 34, no. 2, pp. 786–797, 1991.
[106] N. Wale, I. A. Watson, and G. Karypis, “Comparison
of descriptor spaces for chemical compound retrieval and
classification,” Knowledge and Information Systems , vol. 14,
no. 3, pp. 347–375, Mar. 2008. [Online]. Available:
http://link.springer.com/10.1007/s10115-007-0103-5
[107] H. Toivonen, A. Srinivasan, R. D. King, S. Kramer,
and C. Helma, “Statistical evaluation of the Predictive
ToxicologyChallenge 2000–2001,” Bioinformatics , vol. 19,
no. 10, pp. 1183–1193, Jul. 2003. [Online]. Available:
https://academic.oup.com/bioinformatics/article/19/10/1183/184239
[108] X. Kong, J. Zhang, and P. S. Yu, “Inferring anchor links
across multiple heterogeneous social networks,” in Proceedings
of the 22nd ACM international conference on Conference on
information & knowledge management - CIKM ’13 . San Francisco,California, USA: ACM Press, 2013, pp. 179–188. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=2505515.2505531
[109] M. Wan and J. McAuley, “Item recommendation on monotonic
behavior chains,” in Proceedings of the 12th ACM Conference on
Recommender Systems . Vancouver British Columbia Canada: ACM,
Sep. 2018, pp. 86–94. [Online]. Available: https://dl.acm.org/doi/10.
1145/3240323.3240369
[110] J. Ni, J. Li, and J. McAuley, “Justifying Recommendations
using Distantly-Labeled Reviews and Fine-Grained Aspects,” in
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) . Hong Kong,
China: Association for Computational Linguistics, 2019, pp. 188–197.
[Online]. Available: https://www.aclweb.org/anthology/D19-1018
[111] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,
R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
1024–1034. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html
[112] A. Bojchevski and S. G ¨unnemann, “Deep Gaussian Embedding of
Graphs: Unsupervised Inductive Learning via Ranking,” Feb. 2018,
arXiv:1707.03815 [cs, stat]. [Online]. Available: http://arxiv.org/abs/
1707.03815
[113] O. Shchur, M. Mumme, A. Bojchevski, and S. G ¨unnemann, “Pitfalls
of Graph Neural Network Evaluation,” Jun. 2019, arXiv:1811.05868
[cs, stat]. [Online]. Available: http://arxiv.org/abs/1811.05868
[114] R. Rossi and N. Ahmed, “The Network Data Repository with
Interactive Graph Analytics and Visualization,” Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 29, no. 1, Mar.
2015. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
view/9277
[115] H. Huang, H. Wang, and X. Wang, “An analysis framework
of research frontiers based on the large-scale open academic
graph,” Proceedings of the Association for Information Science and
Technology , vol. 57, no. 1, p. e307, Oct. 2020. [Online]. Available:
https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.307
[116] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and
J. Schulman, “Training Verifiers to Solve Math Word Problems,” Nov.
2021, arXiv:2110.14168 [cs]. [Online]. Available: http://arxiv.org/abs/
2110.14168
[117] A. Patel, S. Bhattamishra, and N. Goyal, “Are NLP Models really able
to Solve Simple Math Word Problems?” Apr. 2021, arXiv:2103.07191
[cs]. [Online]. Available: http://arxiv.org/abs/2103.07191
[118] S. Han, H. Schoelkopf, Y . Zhao, Z. Qi, M. Riddell, L. Benson,
L. Sun, E. Zubova, Y . Qiao, M. Burtell, D. Peng, J. Fan, Y . Liu,
B. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang,
S. Joty, A. R. Fabbri, W. Kryscinski, X. V . Lin, C. Xiong, and
D. Radev, “FOLIO: Natural Language Reasoning with First-Order
Logic,” Sep. 2022, arXiv:2209.00840 [cs]. [Online]. Available:
http://arxiv.org/abs/2209.00840
[119] Y . Zhang, B. Jin, Q. Zhu, Y . Meng, and J. Han, “The effect of metadata
on scientific literature tagging: A cross-field cross-model study,” in
Proceedings of the ACM Web Conference 2023 , 2023, pp. 1626–1637.
[120] A. Johnson, T. Pollard, and R. Mark, “MIMIC-III Clinical Database,”
2015. [Online]. Available: https://physionet.org/content/mimiciii/1.4/
[121] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor,
“Freebase: a collaboratively created graph database for structuring
human knowledge,” in Proceedings of the 2008 ACM SIGMOD
international conference on Management of data . Vancouver
Canada: ACM, Jun. 2008, pp. 1247–1250. [Online]. Available:
https://dl.acm.org/doi/10.1145/1376616.1376746
[122] K. Toutanova and D. Chen, “Observed versus latent features
for knowledge base and text inference,” in Proceedings of the
3rd Workshop on Continuous Vector Space Models and their
Compositionality . Beijing, China: Association for Computational
Linguistics, 2015, pp. 57–66. [Online]. Available: http://aclweb.org/
anthology/W15-4007
[123] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh, “The
value of semantic parse labeling for knowledge base question
answering,” in Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics, ACL 2016, August
7-12, 2016, Berlin, Germany, Volume 2: Short Papers . The
Association for Computer Linguistics, 2016. [Online]. Available:
https://doi.org/10.18653/v1/p16-2033
[124] A. Talmor and J. Berant, “The Web as a Knowledge-base for
Answering Complex Questions,” Mar. 2018, arXiv:1803.06643 [cs].
[Online]. Available: http://arxiv.org/abs/1803.06643
[125] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”
inText summarization branches out , 2004, pp. 74–81.
[126] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in Proceedings
of the 40th Annual Meeting on Association for Computational
Linguistics - ACL ’02 . Philadelphia, Pennsylvania: Association
for Computational Linguistics, 2001, p. 311. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=1073083.1073135
[127] C. Goutte and ´E. Gaussier, “A probabilistic interpretation of precision,
recall and F-score, with implication for evaluation,” in Advances in
Information Retrieval, 27th European Conference on IR Research,
ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005,
Proceedings , ser. Lecture Notes in Computer Science, D. E. Losada and
J. M. Fern ´andez-Luna, Eds., vol. 3408. Springer, 2005, pp. 345–359.
[Online]. Available: https://doi.org/10.1007/978-3-540-31865-1 25
[128] J. M. Kleinberg, R. Kumar, P. Raghavan, S. Rajagopalan, and
A. Tomkins, “The web as a graph: Measurements, models,
and methods,” in Computing and Combinatorics, 5th Annual
International Conference, COCOON ’99, Tokyo, Japan, July 26-
28, 1999, Proceedings , ser. Lecture Notes in Computer Science,
T. Asano, H. Imai, D. T. Lee, S. Nakano, and T. Tokuyama,
Eds., vol. 1627. Springer, 1999, pp. 1–17. [Online]. Available:
https://doi.org/10.1007/3-540-48686-0 1
[129] S. M. Iacus, G. King, and G. Porro, “Causal inference without balance
checking: Coarsened exact matching,” Political analysis , vol. 20, no. 1,
pp. 1–24, 2012.
[130] H. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z. Deng, L. Kong, and
Q. Liu, “GIMLET: A unified graph-text model for instruction-
based molecule zero-shot learning,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
129033c7c08be683059559e8d6bfd460-Abstract-Conference.html