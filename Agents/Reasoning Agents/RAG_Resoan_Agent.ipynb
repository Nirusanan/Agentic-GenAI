{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "use Deepseek models with Autogen Agents through Groq integration for basic queries, RAG applications, and interactive UI implementations."
      ],
      "metadata": {
        "id": "dHyiwHpr2NEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFjjHJpC1_4y"
      },
      "outputs": [],
      "source": [
        "! pip install -q  \"groq==0.18.0\" \"pyautogen==0.7.6\" \"pyautogen[retrievechat]\" chromadb streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import autogen\n",
        "import chromadb\n",
        "from autogen import AssistantAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSVtVjG72JZL",
        "outputId": "532fe5d8-0d88-40bd-c5c9-fe249cdf7499"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "model = \"deepseek-r1-distill-qwen-32b\" #deepseek-r1-distill-llama-70b"
      ],
      "metadata": {
        "id": "SKbXWrvr2JP1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AUTOGEN_USE_DOCKER\"] = \"False\""
      ],
      "metadata": {
        "id": "4zEtg2E_2JNl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_list = [\n",
        "    {\n",
        "    \"model\": model,\n",
        "    \"api_key\": api_key,\n",
        "    \"api_type\": \"groq\",\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "XrPUvbyz6pKA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config = {\n",
        "    \"timeout\": 60,\n",
        "    \"cache_seed\": 42,\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0,\n",
        "}"
      ],
      "metadata": {
        "id": "X1bB3X_D6q6n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def termination_msg(x):\n",
        "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
        "\n",
        "URL = \"/content/parametric-RAG.pdf\"\n",
        "PROBLEM = \"What is Parametric Retrieval Augmented Generation in one line?\""
      ],
      "metadata": {
        "id": "hjsvF3eZAXnZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boss agent - without RAG\n",
        "boss = autogen.UserProxyAgent(\n",
        "    name=\"Boss\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    system_message=\"The boss who ask questions and give tasks.\",\n",
        "    code_execution_config=False,\n",
        "    default_auto_reply=\"Reply `TERMINATE` if the task is done.\",\n",
        ")\n",
        "\n",
        "# boss_aid agent - with RAg\n",
        "boss_aid = RetrieveUserProxyAgent(\n",
        "    name=\"Boss_Assistant\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=3,\n",
        "    retrieve_config={\n",
        "        \"task\": \"code\",\n",
        "        \"docs_path\": URL,\n",
        "        \"chunk_token_size\": 1000,\n",
        "        \"model\": config_list[0][\"model\"],\n",
        "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
        "        \"collection_name\": \"groupchat\",\n",
        "        \"get_or_create\": True,\n",
        "    },\n",
        "    code_execution_config=False,\n",
        ")\n",
        "\n",
        "\n",
        "coder = AssistantAgent(\n",
        "    name=\"Senior_Python_Engineer\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "\n",
        "def _reset_agents():\n",
        "    boss.reset()\n",
        "    boss_aid.reset()\n",
        "    coder.reset()"
      ],
      "metadata": {
        "id": "6_6hPzhY7GXv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "687ccd7758354dbeb2e4ba0b51684c68",
            "5f86f96c92104ffbb66758a5ffaa8dbc",
            "08f093d42e774834a667ca685fe418ee",
            "4319e847ce5f472ba670fe9782bf4d1f",
            "7c70cf0cc8ff488cb882a5dd047cbdaf",
            "fa29b3ee9a6e44b588cc73570c1f482a",
            "d72083120fdd4500a64fb70e486b1a29",
            "213948b1806c4eeaaf63fef9ed06761f",
            "e3c212151aeb4c55a7a403a623a86e3c",
            "2043ac9867bd4b3eab84a6ecfe6a892a",
            "7ca867954a6b4c54a223a5d2b5ce9b99",
            "1c5e1fe8b10a49078d829821a37cba6b",
            "796d8acf39774c5283623d4a2bff34f1",
            "4ba993641442424387f566771895b633",
            "7a628bd2382c43d6810128d48861897e",
            "539ad7bafba146ddba4cbc0ffeb4f94a",
            "6f958eea2a12451c8c8cb9e4a8ce6806",
            "bef156a2667343839bdf26fe65cafc58",
            "d53156c475cf4e76bf0df4eddf75eb68",
            "8bd7d3eceab0483da44357ba86546e69",
            "688bf940a7214fc9bf2fb63f2cd98b6f",
            "2783c58c9dba4439a80efd470f71a8d7",
            "f26cca605e654d19ac0619f8eccc9649",
            "fb72300f298f40e5a6154531a0c6c3c9",
            "e3f5edf72ab44435b2e7b3bc696fd785",
            "f7f82affc00c4702af955ca11398c115",
            "109a5398d6a246098b33b23ec63d3349",
            "41f1610b53964736bbe58c7804eb6a4d",
            "7a5c3187346541bbbc946fac4ef72153",
            "afcdd94e73de48de99582e67a00f3815",
            "c94ad2b7af1f4f59b342eb27b02e2959",
            "aa942b4fbb004885a7d85f9aa6aa2a00",
            "bb18081b2e0146369553c2fb6eec3c2d",
            "1c5055a18dcc4b3fb009b3a3c354fba1",
            "8c764764bfe54a62aa96f773ce733bf8",
            "77a7eefa3f8743dea2c680d1e6ebf0e3",
            "e92b561031df4bb4be48d996561557dd",
            "8c514f9332e542e8942ea752fdb9a0dd",
            "0330daa772634df8839d16c75d74f507",
            "2e1f9ceea23d46c1827a84fb3ec0b301",
            "9003d18874344f8187748516bcbbf6a1",
            "2a29237ed7024c6297dcb81fcdd2a3a7",
            "f38d7e5c5c4240b2b32c04f807e9bc4f",
            "345cf24be4954a5fb0eb10c2bd1d90fd",
            "d4f3fc2dfb6d401284794293a2d28825",
            "3946f42311ea47dca86c3e66c7399253",
            "845d78006c9d4fe59b93affd755c53f4",
            "766c39c348fa42b087f5b37b5421928c",
            "4ed19c0dff404879b4ca9dc6ebf79943",
            "cb04fde8c7174029b4579a73b61b7661",
            "b9cb4bbc4d7f4cbf9ec9ccb49f28de11",
            "c039ad34036f4ab3a4b9f080f11bd036",
            "ad3f2d4eed1d46afbf1da95710bddb71",
            "eedbd31221c7443a8712c8aeb59c3995",
            "6066caa2ee764de59efe3019cd1c3b6c",
            "78fac7330a3442d6a908138b7bc5c23b",
            "f76ffe6647bb4e9e990937d27f5a184e",
            "0ab4e18f13b4455aabb4e036e44f0c0f",
            "6941703d598f4bc9a5a56e8619c57a02",
            "cdcbbf77f5784e1b83e1ee28e947e989",
            "7965a7ea8d004d03b323e12544d0b324",
            "2554f7fcca984fb88bedd00071ef65ea",
            "e035f5196b054840822cd95dc42f462b",
            "8b594069c28846ccba3800f576c93e8c",
            "0195284fb4aa402795592008aebc2437",
            "5d265dc4fe6d4ce6ba58c75f4d2003df",
            "6660a0d15ece4385a6343c23943eca2d",
            "19e3e06fb0fa4dc4b22abc73561dce02",
            "c33d92e9e4f9481c8f249b0c25d32c3b",
            "165c8c28ea144958bdbdbc228d829fe0",
            "73a691a59efd459cb4a32f8c7fcf463f",
            "29defa2e498d442a8396241729654746",
            "4f641ed35383461ba7bb617224919ce4",
            "623d740af2f1495e8dbece2d4952cf9f",
            "a3f685f7d381449aaa14542bd3404f7a",
            "44a8aad66986458db887a9976ddde6a5",
            "aa82bfa42e6b4849b09a865d18044cc9",
            "c9f6b003dfed42b19cca2df4b47155b0",
            "3adb4b5e08d8459685ac73d80adff8a1",
            "7112879ab2bd4e74bb2420c75e16242e",
            "d189fd11a83e407b9c292f4ce945a8b4",
            "19ff950d95064fe1bb4749cc5ee8d098",
            "77097cbbc27240a09894520762025c1a",
            "09c4b83b63984807a4bfed7e0169d8a9",
            "3eb02280b85c4c07a556d8282064d50e",
            "7d8fb866ff8d422ab30438974376be33",
            "6000864e97b341748267afe6c719d7b1",
            "0c6e81fa08854a6080d395ff874217ac",
            "a4de62e2144c48ef8b4ee3cb11342e1b",
            "b8efa401ff3d4c2cbdc1b0fd736d3417",
            "8e08469656754321a2a55f02c553d8fd",
            "4fa7a08fd33245b8a1253a9040b75fa3",
            "2f1ea7401f9c402c86dd0ac443294f53",
            "e4abe7aae9f948a1abda5f6ed08ef538",
            "33592573b6a8422ab9307476c963a580",
            "1cce967507674c5d99c8a349e63e27dc",
            "b469f993b6fd40e5a9396b481d4182c9",
            "fc1a5f3e7d21432f9775f5cd006b2dd8",
            "84e1201a3ced4dd2ab4fe3bc12424831",
            "cc9c9db90da34b608f106c26b83ab0e0",
            "4773832ec9ce43a1ae321a911229a95d",
            "c7394b56884f43f4a3bbce204fa526eb",
            "9971d5fb03e24765b98ef9ac548ea1b9",
            "f9c1bb438dbd4ceeb2f10d960ca9b07b",
            "aeed4be1e63640a3a1236f40190b6e79",
            "8213bf0b75e4487e86d52a597474469f",
            "defc5b07dfac418d89f440dcb553dfb0",
            "65160ca05c42429e80345380daa25993",
            "3e8e8c95930a4619adcd6c1dbf0f8b16",
            "b0e16a4e686c484fa87b74023d07a78f",
            "c44da41d95914bf6bef54fca6801f4c4",
            "17f746ecaf374625a3b28fb0d06952c8",
            "7a35b26be1f142e6822f027a1674d36a",
            "adc8bdc9f47e4612a7d521710ac62fdb",
            "05f9ce04c4c14883a1d0c5e81bccbc0d",
            "3877f9c831a94de39861ed3b1d03add3",
            "9fb597bf396440e89b21b689b3f1508a",
            "e214fb1729794ac5bad69a966fe7ee86",
            "071d390f4fff4dc585905179ca7237a6",
            "56be437592a74e319518d51cf7530ac7",
            "63f9e2d44fe843f2a86fcda744818d06"
          ]
        },
        "outputId": "9d21f44e-8a84-4ed7-f1d1-1e9f55aa0081"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "687ccd7758354dbeb2e4ba0b51684c68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c5e1fe8b10a49078d829821a37cba6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f26cca605e654d19ac0619f8eccc9649"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c5055a18dcc4b3fb009b3a3c354fba1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4f3fc2dfb6d401284794293a2d28825"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78fac7330a3442d6a908138b7bc5c23b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6660a0d15ece4385a6343c23943eca2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9f6b003dfed42b19cca2df4b47155b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4de62e2144c48ef8b4ee3cb11342e1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc9c9db90da34b608f106c26b83ab0e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c44da41d95914bf6bef54fca6801f4c4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without RAG"
      ],
      "metadata": {
        "id": "DrOmuOTXDxbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norag_chat():\n",
        "    _reset_agents()\n",
        "    groupchat = autogen.GroupChat(\n",
        "        agents=[boss, coder],\n",
        "        messages=[],\n",
        "        max_round=12,\n",
        "        speaker_selection_method=\"auto\",\n",
        "        allow_repeat_speaker=False,\n",
        "    )\n",
        "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
        "\n",
        "    # Start chatting with the boss as this is the user proxy agent.\n",
        "    boss.initiate_chat(\n",
        "        manager,\n",
        "        message=PROBLEM,\n",
        "    )\n",
        "\n",
        "norag_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfQsULCiDyRV",
        "outputId": "4d60b43b-9284-40ac-b85c-a14872138ece"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boss (to chat_manager):\n",
            "\n",
            "What is Parametric Retrieval Augmented Generation in one line?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Senior_Python_Engineer\n",
            "\n",
            "Senior_Python_Engineer (to chat_manager):\n",
            "\n",
            "<think>\n",
            "Okay, so I need to figure out what Parametric Retrieval Augmented Generation (Parametric RAG) is. I remember that RAG stands for Retrieval-Augmented Generation, which is a method where a model uses external information to generate responses. But what makes it \"Parametric\"?\n",
            "\n",
            "Hmm, parametric models are those that have a fixed set of parameters, right? So maybe Parametric RAG uses a model that has parameters that can be adjusted or fine-tuned. Wait, but isn't RAG already a type of model that uses parameters? Maybe the difference is in how the retrieval part is handled.\n",
            "\n",
            "In traditional RAG, the model might retrieve information from a database or some external source, like documents or web pages. But Parametric RAG might instead use a parametric approach for the retrieval. So instead of querying an external database, it might use a part of the model itself to retrieve information.\n",
            "\n",
            "I think I've heard about using dense vectors or embeddings for retrieval. Maybe in Parametric RAG, the retrieval is done using a part of the model's parameters, like a memory bank or a knowledge base that's stored within the model's architecture. This could make the model more efficient because it doesn't need to access external data at inference time.\n",
            "\n",
            "So, putting it all together, Parametric RAG is a method where the model uses its own parameters to retrieve relevant information, which is then used to generate more accurate or context-aware responses. This could be useful in scenarios where real-time access to external data is limited or where the model needs to be self-contained.\n",
            "\n",
            "I should also consider how this compares to non-parametric RAG. Non-parametric might rely on external databases, which can be flexible but might introduce latency or require more resources. Parametric RAG, on the other hand, keeps everything within the model, potentially making it faster and more deployable in different environments.\n",
            "\n",
            "Another thought: maybe Parametric RAG uses techniques like memory networks or attention mechanisms to retrieve information from its internal parameters. This would allow the model to dynamically adjust what information it retrieves based on the input, making it more adaptable.\n",
            "\n",
            "I'm also thinking about how this affects training. If the retrieval is part of the model's parameters, then the model would need to be trained in a way that both the retrieval and generation parts are optimized together. This could lead to better integration between the two processes, resulting in more coherent and relevant outputs.\n",
            "\n",
            "In summary, Parametric RAG likely refers to a RAG approach where the retrieval mechanism is integrated into the model's architecture using parameters, allowing for efficient and self-contained information retrieval during generation.\n",
            "</think>\n",
            "\n",
            "Parametric Retrieval Augmented Generation (Parametric RAG) is a method where the model integrates its retrieval mechanism into its architecture using parameters, enabling efficient and self-contained information retrieval during generation. This approach contrasts with traditional RAG, which often relies on external databases, by keeping all necessary information within the model, enhancing efficiency and adaptability.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Boss\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/autogen/oai/groq.py:279: UserWarning: Cost calculation not available for model deepseek-r1-distill-qwen-32b\n",
            "  warnings.warn(f\"Cost calculation not available for model {model}\", UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with RAG"
      ],
      "metadata": {
        "id": "iXMUFFwJEWTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chat():\n",
        "    _reset_agents()\n",
        "    groupchat = autogen.GroupChat(\n",
        "        agents=[boss_aid, coder],\n",
        "        messages=[],\n",
        "        max_round=12,\n",
        "        speaker_selection_method=\"round_robin\"\n",
        "    )\n",
        "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
        "\n",
        "    # Start chatting with boss_aid as this is the user proxy agent.\n",
        "    boss_aid.initiate_chat(\n",
        "        manager,\n",
        "        message=PROBLEM,\n",
        "        n_results=3,\n",
        "    )\n",
        "\n",
        "rag_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm1RYImv7VZJ",
        "outputId": "03f198f8-e983-4725-bd5a-b1ee9945978f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boss_Assistant (to chat_manager):\n",
            "\n",
            "What is Parametric Retrieval Augmented Generation in one line?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Senior_Python_Engineer\n",
            "\n",
            "Senior_Python_Engineer (to chat_manager):\n",
            "\n",
            "<think>\n",
            "Okay, so I need to figure out what Parametric Retrieval Augmented Generation (Parametric RAG) is. I remember that RAG stands for Retrieval-Augmented Generation, which is a method where a model uses external information to generate responses. But what makes it \"Parametric\"?\n",
            "\n",
            "Hmm, parametric models are those that have a fixed set of parameters, right? So maybe Parametric RAG uses a model that has parameters that can be adjusted or fine-tuned. Wait, but isn't RAG already using a model with parameters? So maybe the difference is in how the retrieval part is handled.\n",
            "\n",
            "In traditional RAG, the model retrieves information from a database or corpus, which is often non-parametric because it's based on stored data. But Parametric RAG might instead use a parametric approach for retrieval, meaning the retrieval process is integrated into the model's parameters. So instead of querying an external database, the model might have a built-in mechanism to retrieve information from its own parameters or a knowledge base that's part of the model.\n",
            "\n",
            "I think this could make the model more efficient because it doesn't rely on external lookups, which can be slow or require additional resources. It might also allow for better integration with the generation process, making the entire system more cohesive.\n",
            "\n",
            "So putting it all together, Parametric RAG is a method where the retrieval of information is handled through the model's parameters, allowing for a more integrated and efficient generation process compared to traditional RAG which uses external retrieval sources.\n",
            "</think>\n",
            "\n",
            "Parametric Retrieval Augmented Generation (Parametric RAG) is a method where the retrieval of information is integrated into the model's parameters, enabling a more efficient and cohesive generation process compared to traditional RAG, which relies on external retrieval sources.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Boss_Assistant\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
