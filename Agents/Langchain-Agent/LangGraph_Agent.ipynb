{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6itOpJk1OJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84177ce-0f18-4612-c428-b36dbb477595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q langchain langgraph langchain-groq duckduckgo-search beautifulsoup4 gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools, operator, requests, os, json\n",
        "from bs4 import BeautifulSoup\n",
        "from duckduckgo_search import DDGS\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.tools import tool\n",
        "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "cs0_zMsW1oTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "jy74KpnfYmEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Groq_api_key = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "vKszl4QIZ0sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "available models: llama3-8b-8192, llama3-70b-8192,  mixtral-8x7b-32768, llama-3.3-70b-versatile"
      ],
      "metadata": {
        "id": "_Dwk_-Oi2aLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(temperature=0, groq_api_key=Groq_api_key, model_name=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "V1fPJl2HZP1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### custom tools"
      ],
      "metadata": {
        "id": "8ibrNH_J5JDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"internet_search\", return_direct=False)\n",
        "def internet_search(query: str) -> str:\n",
        "    \"\"\"Searches the internet using DuckDuckGo.\"\"\"\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            results = [r for r in ddgs.text(query, max_results=3)]\n",
        "            return results if results else \"No results found.\"\n",
        "    except ValueError as e:\n",
        "        return f\"Search failed: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {str(e)}\"\n",
        "\n",
        "@tool(\"process_content\", return_direct=False)\n",
        "def process_content(url: str) -> str:\n",
        "    \"\"\"Processes content from a webpage.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "tools = [internet_search, process_content]"
      ],
      "metadata": {
        "id": "2bKoolFs13t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent(llm: ChatGroq, tools: list, system_prompt: str):\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor"
      ],
      "metadata": {
        "id": "EaIQ3fl65Pv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agent Nodes"
      ],
      "metadata": {
        "id": "dIzY8DaS5VKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
      ],
      "metadata": {
        "id": "atA6ahib5TPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agent Supervisor"
      ],
      "metadata": {
        "id": "2qoUQQNl6LTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "members = [\"Web_Searcher\", \"Insight_Researcher\"]\n",
        "system_prompt = (\n",
        "    \"As a supervisor, your role is to oversee a dialogue between these\"\n",
        "    \" workers: {members}. Based on the user's request,\"\n",
        "    \" determine which worker should take the next action. Each worker is responsible for\"\n",
        "    \" executing a specific task and reporting back their findings and progress. Once all tasks are complete,\"\n",
        "    \" indicate with 'FINISH'.\"\n",
        ")\n",
        "\n",
        "options = [\"FINISH\"] + members\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"Select the next role.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\"next\": {\"title\": \"Next\", \"anyOf\": [{\"enum\": options}] }},\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    (\"system\", \"Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}\"),\n",
        "]).partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "supervisor_chain = (prompt | llm.bind_functions(functions=[function_def], function_call=\"route\") | JsonOutputFunctionsParser())"
      ],
      "metadata": {
        "id": "zCz3KdNN5YLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687ad2d5-3fb5-4365-b394-63949abf4387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-9aa83a6ca0c2>:28: LangChainDeprecationWarning: The method `ChatGroq.bind_functions` was deprecated in langchain-groq 0.2.1 and will be removed in 1.0.0. Use :meth:`~langchain_groq.chat_models.ChatGroq.bind_tools` instead.\n",
            "  supervisor_chain = (prompt | llm.bind_functions(functions=[function_def], function_call=\"route\") | JsonOutputFunctionsParser())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agent State and Graph"
      ],
      "metadata": {
        "id": "CmR5bPLg6YZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    next: str\n",
        "\n",
        "search_agent = create_agent(llm, tools, \"You are a web searcher. Search the internet for information.\")\n",
        "search_node = functools.partial(agent_node, agent=search_agent, name=\"Web_Searcher\")\n",
        "\n",
        "insights_research_agent = create_agent(llm, tools,\n",
        "        \"\"\"You are a Insight Researcher. Do step by step.\n",
        "        Based on the provided content first identify the list of topics,\n",
        "        then search internet for each topic one by one\n",
        "        and finally find insights for each topic one by one.\n",
        "        Include the insights and sources in the final response\n",
        "        \"\"\")\n",
        "insights_research_node = functools.partial(agent_node, agent=insights_research_agent, name=\"Insight_Researcher\")\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"Web_Searcher\", search_node)\n",
        "workflow.add_node(\"Insight_Researcher\", insights_research_node)\n",
        "workflow.add_node(\"supervisor\", supervisor_chain)\n"
      ],
      "metadata": {
        "id": "quoEykrQ6V7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1030fb-84f2-4f55-ca3b-4c1e43596627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7de31846bd90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Egdes"
      ],
      "metadata": {
        "id": "qj1pDSA66dvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for member in members:\n",
        "    workflow.add_edge(member, \"supervisor\")\n",
        "\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map[\"FINISH\"] = END\n",
        "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
        "workflow.set_entry_point(\"supervisor\")\n",
        "\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "vsEv9Hty6c6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### gradio"
      ],
      "metadata": {
        "id": "BgdR0BE76snD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_graph(input_message):\n",
        "    response = graph.invoke({\n",
        "        \"messages\": [HumanMessage(content=input_message)]\n",
        "    })\n",
        "    return json.dumps(response['messages'][1].content, indent=2)\n",
        "\n",
        "inputs = gr.Textbox(lines=2, placeholder=\"Enter your query here...\")\n",
        "outputs = gr.Textbox()\n",
        "title=\"LangGraph DuckDuckGo-Search\"\n",
        "\n",
        "demo = gr.Interface(fn=run_graph, inputs=inputs, outputs=outputs, title=title)\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "RcUzOrK86jnl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}