{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSQNaIkry49C",
        "outputId": "beb5aab6-857a-4aa8-972e-d7e0589689eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.6/419.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q  \"groq==0.13.0\" \"ag2==0.9.6\" \"ag2[retrievechat]\" chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYxxjtkUy7fc",
        "outputId": "1d0cc762-e69e-49a9-d02e-e4ee0a1a2a7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import autogen\n",
        "import chromadb\n",
        "from autogen import AssistantAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zkNYk2hKy7c8"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "model = \"llama-3.3-70b-versatile\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H29vH3Rr0pFk"
      },
      "outputs": [],
      "source": [
        "os.environ[\"AUTOGEN_USE_DOCKER\"] = \"False\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "coZmNKRljVVv"
      },
      "outputs": [],
      "source": [
        "config_list = [\n",
        "    {\n",
        "    \"model\": model,\n",
        "    \"api_key\": api_key,\n",
        "    \"api_type\": \"groq\",\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VEVHee-F0K3i"
      },
      "outputs": [],
      "source": [
        "llm_config = {\n",
        "    \"timeout\": 60,\n",
        "    \"cache_seed\": 42,\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4PPoNr3HcsDJ"
      },
      "outputs": [],
      "source": [
        "def termination_msg(x):\n",
        "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
        "\n",
        "URL = \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\"\n",
        "PROBLEM = \"How to use spark for parallel training in FLAML? Give me sample code.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FIw3_THDy7a6"
      },
      "outputs": [],
      "source": [
        "# boss agent - without RAG\n",
        "boss = autogen.UserProxyAgent(\n",
        "    name=\"Boss\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    system_message=\"The boss who ask questions and give tasks.\",\n",
        "    code_execution_config=False,\n",
        "    default_auto_reply=\"Reply `TERMINATE` if the task is done.\",\n",
        ")\n",
        "\n",
        "# boss_aid agent - with RAg\n",
        "boss_aid = RetrieveUserProxyAgent(\n",
        "    name=\"Boss_Assistant\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=3,\n",
        "    retrieve_config={\n",
        "        \"task\": \"code\",\n",
        "        \"docs_path\": URL,\n",
        "        \"chunk_token_size\": 1000,\n",
        "        \"model\": config_list[0][\"model\"],\n",
        "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
        "        \"collection_name\": \"groupchat\",\n",
        "        \"get_or_create\": True,\n",
        "    },\n",
        "    code_execution_config=False,\n",
        ")\n",
        "\n",
        "\n",
        "coder = AssistantAgent(\n",
        "    name=\"Senior_Python_Engineer\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "\n",
        "pm = autogen.AssistantAgent(\n",
        "    name=\"Product_Manager\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "\n",
        "reviewer = autogen.AssistantAgent(\n",
        "    name=\"Code_Reviewer\",\n",
        "    is_termination_msg=termination_msg,\n",
        "    system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "\n",
        "def _reset_agents():\n",
        "    boss.reset()\n",
        "    boss_aid.reset()\n",
        "    coder.reset()\n",
        "    pm.reset()\n",
        "    reviewer.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4i8AYJzflzy"
      },
      "source": [
        "#### without RAG - No RAG chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LALL9feOfh0Z",
        "outputId": "246762a8-5dba-4907-b733-cc5081d2536a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boss (to chat_manager):\n",
            "\n",
            "How to use spark for parallel training in FLAML? Give me sample code.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Senior_Python_Engineer\n",
            "\n",
            "Senior_Python_Engineer (to chat_manager):\n",
            "\n",
            "FLAML (Fast and Lightweight Automated Machine Learning) is a Python library that provides an efficient way to perform automated machine learning. To use Spark for parallel training in FLAML, you can leverage the `spark` backend provided by FLAML.\n",
            "\n",
            "Here's a sample code snippet that demonstrates how to use Spark for parallel training in FLAML:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from flaml import AutoML\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "# Create a SparkSession\n",
            "spark = SparkSession.builder.appName(\"FLAML Spark Example\").getOrCreate()\n",
            "\n",
            "# Load your dataset into a Pandas DataFrame\n",
            "df = pd.read_csv(\"your_data.csv\")\n",
            "\n",
            "# Define the task and the time budget\n",
            "task = \"classification\"\n",
            "time_budget = 3600  # in seconds\n",
            "\n",
            "# Create an AutoML instance with the Spark backend\n",
            "automl = AutoML()\n",
            "automl.fit(df, task=task, time_budget=time_budget, backend=\"spark\")\n",
            "\n",
            "# Get the best model and its performance\n",
            "best_model, best_model_performances = automl.get_best_model()\n",
            "\n",
            "# Print the best model's performance\n",
            "print(best_model_performances)\n",
            "\n",
            "# Stop the SparkSession\n",
            "spark.stop()\n",
            "```\n",
            "\n",
            "In this example, we first create a SparkSession, which is the entry point to programming Spark with the Dataset and DataFrame API. We then load our dataset into a Pandas DataFrame.\n",
            "\n",
            "Next, we define the task (classification or regression) and the time budget for the automated machine learning process.\n",
            "\n",
            "We create an AutoML instance and call the `fit` method, passing in the dataset, task, time budget, and the `backend=\"spark\"` argument to enable Spark-based parallel training.\n",
            "\n",
            "After the `fit` method completes, we can retrieve the best model and its performance using the `get_best_model` method.\n",
            "\n",
            "Finally, we stop the SparkSession to release any allocated resources.\n",
            "\n",
            "Note that you need to have Spark installed and configured properly on your system for this code to work. Additionally, you may need to adjust the Spark configuration, such as the number of executor cores, to optimize performance for your specific use case.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Boss\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def norag_chat():\n",
        "    _reset_agents()\n",
        "    groupchat = autogen.GroupChat(\n",
        "        agents=[boss, coder, pm, reviewer], # without RAG - use boss agent\n",
        "        messages=[],\n",
        "        max_round=12,\n",
        "        speaker_selection_method=\"auto\",\n",
        "        allow_repeat_speaker=False,\n",
        "    )\n",
        "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
        "\n",
        "    # Start chatting with the boss as this is the user proxy agent.\n",
        "    boss.initiate_chat(\n",
        "        manager,\n",
        "        message=PROBLEM,\n",
        "    )\n",
        "\n",
        "norag_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGpwds08cTUf"
      },
      "source": [
        "#### with RAG (RAG chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvf8SfDcL9Y",
        "outputId": "948847b5-9f03-48f4-be6a-44a085a28951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boss_Assistant (to chat_manager):\n",
            "\n",
            "How to use spark for parallel training in FLAML? Give me sample code.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Senior_Python_Engineer\n",
            "\n",
            "Senior_Python_Engineer (to chat_manager):\n",
            "\n",
            "FLAML (Fast and Lightweight Automated Machine Learning) is a Python library that provides an efficient way to perform automated machine learning. To use Spark for parallel training in FLAML, you can leverage the `spark` backend provided by FLAML.\n",
            "\n",
            "Here's a sample code snippet that demonstrates how to use Spark for parallel training in FLAML:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from flaml import AutoML\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "# Create a SparkSession\n",
            "spark = SparkSession.builder.appName(\"FLAML Spark Example\").getOrCreate()\n",
            "\n",
            "# Load your dataset into a Pandas DataFrame\n",
            "df = pd.read_csv(\"your_data.csv\")\n",
            "\n",
            "# Define the task and the time budget\n",
            "task = \"classification\"\n",
            "time_budget = 3600  # in seconds\n",
            "\n",
            "# Create an AutoML instance with the Spark backend\n",
            "automl = AutoML()\n",
            "automl.fit(df, task=task, time_budget=time_budget, backend=\"spark\", spark_session=spark)\n",
            "\n",
            "# Get the best model and its performance\n",
            "best_model, best_model_performances = automl.get_best_model_with_performance()\n",
            "\n",
            "# Print the best model and its performance\n",
            "print(\"Best Model:\", best_model)\n",
            "print(\"Best Model Performance:\", best_model_performances)\n",
            "\n",
            "# Stop the SparkSession\n",
            "spark.stop()\n",
            "```\n",
            "\n",
            "In this example, we first create a SparkSession using the `SparkSession.builder` API. We then load our dataset into a Pandas DataFrame using `pd.read_csv`. We define the task as classification and set a time budget of 3600 seconds.\n",
            "\n",
            "Next, we create an AutoML instance and call the `fit` method, passing in the dataset, task, time budget, and the Spark backend. We also pass in the SparkSession instance using the `spark_session` parameter.\n",
            "\n",
            "After the `fit` method completes, we can retrieve the best model and its performance using the `get_best_model_with_performance` method. Finally, we stop the SparkSession using the `stop` method.\n",
            "\n",
            "Note that you need to have Spark installed and configured on your system to run this example. Additionally, you may need to adjust the Spark configuration and the time budget to suit your specific use case.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Product_Manager\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autogen/oai/groq.py:283: UserWarning: Cost calculation not available for model llama-3.3-70b-versatile\n",
            "  warnings.warn(f\"Cost calculation not available for model {model}\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "def rag_chat():\n",
        "    _reset_agents()\n",
        "    groupchat = autogen.GroupChat(\n",
        "        agents=[boss_aid, coder, pm, reviewer], # you want RAG, use boss_aid\n",
        "        messages=[],\n",
        "        max_round=12,\n",
        "        speaker_selection_method=\"round_robin\"\n",
        "    )\n",
        "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
        "\n",
        "    # Start chatting with boss_aid as this is the user proxy agent.\n",
        "    boss_aid.initiate_chat(\n",
        "        manager,\n",
        "        message=PROBLEM,\n",
        "        n_results=3,\n",
        "    )\n",
        "\n",
        "rag_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2p3WIMGcgYF"
      },
      "source": [
        "#### FUNCTION CALLING RAG CHAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtEwzFWzcc3r",
        "outputId": "853790c7-9213-4114-92a1-bbbdfa0af027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FUNCTION CALLING RAG CHAT\n",
            "Boss (to chat_manager):\n",
            "\n",
            "How to use spark for parallel training in FLAML? Give me sample code. Must retrieve content using function calling.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Product_Manager\n",
            "\n",
            "Product_Manager (to chat_manager):\n",
            "\n",
            "To use Spark for parallel training in FLAML, you can leverage the `SparkEstimator` class provided by FLAML. This allows you to distribute the training process across multiple nodes in a Spark cluster, significantly speeding up the hyperparameter tuning process for your machine learning models.\n",
            "\n",
            "First, ensure you have FLAML and the necessary Spark dependencies installed in your environment. You can install FLAML using pip:\n",
            "\n",
            "```bash\n",
            "pip install flaml\n",
            "```\n",
            "\n",
            "For Spark, if you haven't installed it yet, you can do so by installing `pyspark`:\n",
            "\n",
            "```bash\n",
            "pip install pyspark\n",
            "```\n",
            "\n",
            "Here's a sample code snippet demonstrating how to use Spark for parallel training in FLAML. This example assumes you're working with a simple classification problem using the `iris` dataset from `sklearn.datasets`.\n",
            "\n",
            "```python\n",
            "from flaml import SparkEstimator\n",
            "from flaml.tune import Tune\n",
            "from pyspark.sql import SparkSession\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "# Initialize Spark Session\n",
            "spark = SparkSession.builder.appName(\"FLAML_Spark_Demo\").getOrCreate()\n",
            "\n",
            "# Load iris dataset\n",
            "iris = load_iris()\n",
            "X = iris.data\n",
            "y = iris.target\n",
            "\n",
            "# Split dataset into training and test sets\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
            "\n",
            "# Convert data to pandas DataFrame\n",
            "df_train = pd.DataFrame(X_train, columns=iris.feature_names)\n",
            "df_train['target'] = y_train\n",
            "\n",
            "# Define a function to train and evaluate a model\n",
            "def train_evaluate(config):\n",
            "    from sklearn.ensemble import RandomForestClassifier\n",
            "    from sklearn.metrics import accuracy_score\n",
            "    model = RandomForestClassifier(n_estimators=config[\"n_estimators\"], max_depth=config[\"max_depth\"])\n",
            "    model.fit(X_train, y_train)\n",
            "    y_pred = model.predict(X_test)\n",
            "    return {\"metric\": accuracy_score(y_test, y_pred), \"loss\": -accuracy_score(y_test, y_pred)}\n",
            "\n",
            "# Define search space\n",
            "search_space = {\n",
            "    \"n_estimators\": {\"type\": \"int\", \"low\": 10, \"high\": 100, \"step\": 10},\n",
            "    \"max_depth\": {\"type\": \"int\", \"low\": 5, \"high\": 20, \"step\": 1},\n",
            "}\n",
            "\n",
            "# Perform hyperparameter tuning using FLAML with Spark\n",
            "tune = Tune()\n",
            "tune.fit(\n",
            "    train_evaluate,\n",
            "    search_space,\n",
            "    estimator=SparkEstimator(num_samples=-1, spark_session=spark),\n",
            "    time_budget=3600,  # Time budget in seconds\n",
            "    verbose=1,\n",
            ")\n",
            "\n",
            "# Retrieve the best configuration and its corresponding metric\n",
            "best_config = tune.best_config\n",
            "best_metric = tune.best_result[\"metric\"]\n",
            "\n",
            "print(f\"Best Configuration: {best_config}\")\n",
            "print(f\"Best Metric (Accuracy): {best_metric}\")\n",
            "\n",
            "# Clean up\n",
            "spark.stop()\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: Senior_Python_Engineer\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from autogen import LLMConfig\n",
        "\n",
        "def function_calling_rag_chat():\n",
        "    _reset_agents()\n",
        "    def retrieve_content(message, n_results=3):\n",
        "        boss_aid.n_results = n_results  # Set the number of results to be retrieved.\n",
        "        # Check if we need to update the context.\n",
        "        update_context_case1, update_context_case2 = boss_aid._check_update_context(message)\n",
        "        if (update_context_case1 or update_context_case2) and boss_aid.update_context:\n",
        "            boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem\n",
        "            _, ret_msg = boss_aid._generate_retrieve_user_reply(message)\n",
        "        else:\n",
        "            ret_msg = boss_aid.generate_init_message(message, n_results=n_results)\n",
        "        return ret_msg if ret_msg else message\n",
        "\n",
        "    boss_aid.human_input_mode = \"NEVER\"  # Disable human input for boss_aid since it only retrieves content.\n",
        "\n",
        "    llm_config = {\n",
        "        \"functions\": [\n",
        "            {\n",
        "                \"name\": \"retrieve_content\",\n",
        "                \"description\": \"retrieve content for code generation and question answering.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"message\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"message\"],\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "        \"config_list\": config_list,\n",
        "        \"timeout\": 60,\n",
        "        \"cache_seed\": 42,\n",
        "    }\n",
        "\n",
        "    for agent in [coder, pm, reviewer]:\n",
        "        # update llm_config for assistant agents.\n",
        "        # agent.llm_config.update(llm_config)\n",
        "        agent.llm_config = LLMConfig(**llm_config)\n",
        "\n",
        "    for agent in [boss, coder, pm, reviewer]:\n",
        "        # register functions for all agents.\n",
        "        agent.register_function(\n",
        "            function_map={\n",
        "                \"retrieve_content\": retrieve_content,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    groupchat = autogen.GroupChat(\n",
        "        agents=[boss, coder, pm, reviewer],\n",
        "        messages=[],\n",
        "        max_round=12,\n",
        "        speaker_selection_method=\"random\",\n",
        "        allow_repeat_speaker=False,\n",
        "    )\n",
        "\n",
        "    manager_llm_config = llm_config.copy()\n",
        "    manager_llm_config.pop(\"functions\")\n",
        "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=manager_llm_config)\n",
        "\n",
        "    # Start chatting with the boss as this is the user proxy agent.\n",
        "    boss.initiate_chat(\n",
        "        manager,\n",
        "        message=PROBLEM+\" Must retrieve content using function calling.\",\n",
        "    )\n",
        "\n",
        "print(\"FUNCTION CALLING RAG CHAT\")\n",
        "function_calling_rag_chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
